# ë°ì´í„°ë² ì´ìŠ¤ ë° ìºì‹± ì‹œìŠ¤í…œ ë³‘ëª©ì  ìƒì„¸ ë¶„ì„

**ì‘ì„±ì¼**: 2025-07-11  
**ë²”ìœ„**: MongoDB, Redis ìºì‹± ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”  
**ëª©ì **: ë°ì´í„°ë² ì´ìŠ¤ ë° ìºì‹± ì˜ì—­ ë³‘ëª©ì  ìƒì„¸ ë¶„ì„ ë° êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ ì œì‹œ

## ğŸ“‹ ë¶„ì„ ê°œìš”

XAI Community v5ì˜ MongoDB ë°ì´í„°ë² ì´ìŠ¤ì™€ Redis ìºì‹± ì‹œìŠ¤í…œì—ì„œ ë°œê²¬ëœ ì£¼ìš” ì„±ëŠ¥ ë³‘ëª©ì ë“¤ì„ ì¸ë±ìŠ¤ ìµœì í™”, ìºì‹± ì „ëµ, ì—°ê²° í’€ë§, ì§‘ê³„ íŒŒì´í”„ë¼ì¸, ìºì‹œ ë¬´íš¨í™” ì˜ì—­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤.

## ğŸ” 1. MongoDB ì¸ë±ìŠ¤ ìµœì í™” ë¶„ì„

### 1.1 í˜„ì¬ ì¸ë±ìŠ¤ êµ¬ì¡° ë¶„ì„

#### ğŸ“Š User ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤ í˜„í™©

**í˜„ì¬ ì¸ë±ìŠ¤:**
```javascript
// ê¸°ì¡´ ì¸ë±ìŠ¤
db.users.createIndex({ "email": 1 }, { unique: true })
db.users.createIndex({ "user_handle": 1 }, { unique: true })
db.users.createIndex({ "status": 1, "created_at": -1 })
```

**ì¸ë±ìŠ¤ ì‚¬ìš© íŒ¨í„´ ë¶„ì„:**
```javascript
// ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ íŒ¨í„´
db.users.find({ "email": "user@example.com" })                    // âœ… ì¸ë±ìŠ¤ ì‚¬ìš©
db.users.find({ "user_handle": "username" })                      // âœ… ì¸ë±ìŠ¤ ì‚¬ìš©
db.users.find({ "status": "active" }).sort({ "created_at": -1 })  // âœ… ì¸ë±ìŠ¤ ì‚¬ìš©

// ğŸš¨ ì¸ë±ìŠ¤ ëˆ„ë½ ì¿¼ë¦¬ íŒ¨í„´
db.users.find({ "display_name": { $regex: /search_term/i } })      // âŒ ì¸ë±ìŠ¤ ì—†ìŒ
db.users.find({ "metadata.location": "Seoul" })                   // âŒ ì¸ë±ìŠ¤ ì—†ìŒ
db.users.find({ "last_login": { $gte: new Date("2025-01-01") } }) // âŒ ì¸ë±ìŠ¤ ì—†ìŒ
```

**ê°œì„  ë°©ì•ˆ:**
```javascript
// ğŸ”§ ì¶”ê°€ ì¸ë±ìŠ¤ ì œì•ˆ
db.users.createIndex({ "display_name": "text" })                  // ì „ë¬¸ ê²€ìƒ‰
db.users.createIndex({ "metadata.location": 1 })                  // ì§€ì—­ ê²€ìƒ‰
db.users.createIndex({ "last_login": -1 })                        // í™œì„± ì‚¬ìš©ì ì¡°íšŒ
db.users.createIndex({ "created_at": -1, "status": 1 })          // ìµœì‹  í™œì„± ì‚¬ìš©ì
```

#### ğŸ“Š Post ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤ í˜„í™©

**í˜„ì¬ ì¸ë±ìŠ¤:**
```javascript
// ê¸°ì¡´ ì¸ë±ìŠ¤
db.posts.createIndex({ "slug": 1 }, { unique: true })
db.posts.createIndex({ "author_id": 1, "created_at": -1 })
db.posts.createIndex({ "service": 1, "status": 1, "created_at": -1 })
```

**ì¿¼ë¦¬ íŒ¨í„´ ë¶„ì„:**
```javascript
// í˜„ì¬ ì§€ì›ë˜ëŠ” ì¿¼ë¦¬
db.posts.find({ "slug": "post-slug" })                           // âœ… ì¸ë±ìŠ¤ ì‚¬ìš©
db.posts.find({ "author_id": ObjectId("...") })                  // âœ… ì¸ë±ìŠ¤ ì‚¬ìš©
db.posts.find({ "service": "board", "status": "active" })        // âœ… ì¸ë±ìŠ¤ ì‚¬ìš©

// ğŸš¨ ì„±ëŠ¥ ì´ìŠˆ ì¿¼ë¦¬ íŒ¨í„´
db.posts.find({ "metadata.type": "question" })                   // âŒ ì¸ë±ìŠ¤ ì—†ìŒ
db.posts.find({ "metadata.category": "tech" })                   // âŒ ì¸ë±ìŠ¤ ì—†ìŒ
db.posts.find({ "metadata.tags": { $in: ["react", "mongodb"] } }) // âŒ ì¸ë±ìŠ¤ ì—†ìŒ
db.posts.find({ "title": { $regex: /search/i } })                // âŒ ì¸ë±ìŠ¤ ì—†ìŒ
```

**ê°œì„  ë°©ì•ˆ:**
```javascript
// ğŸ”§ ë©”íƒ€ë°ì´í„° í•„í„°ë§ ìµœì í™”
db.posts.createIndex({ "metadata.type": 1, "status": 1, "created_at": -1 })
db.posts.createIndex({ "metadata.category": 1, "created_at": -1 })
db.posts.createIndex({ "metadata.tags": 1 })

// ğŸ”§ ë³µí•© ê²€ìƒ‰ ìµœì í™”
db.posts.createIndex({ "service": 1, "metadata.type": 1, "status": 1, "created_at": -1 })

// ğŸ”§ ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤
db.posts.createIndex({ 
  "title": "text", 
  "content": "text", 
  "metadata.tags": "text" 
}, {
  weights: {
    "title": 10,
    "content": 5,
    "metadata.tags": 3
  },
  name: "post_search_index"
})

// ğŸ”§ í†µê³„ ì§‘ê³„ ìµœì í™”
db.posts.createIndex({ "created_at": -1, "status": 1 })          // ìµœì‹  ê²Œì‹œê¸€
db.posts.createIndex({ "view_count": -1, "status": 1 })          // ì¸ê¸° ê²Œì‹œê¸€
```

#### ğŸ“Š Comment ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤ í˜„í™©

**í˜„ì¬ ì¸ë±ìŠ¤:**
```javascript
// ê¸°ì¡´ ì¸ë±ìŠ¤
db.comments.createIndex({ "parent_id": 1, "created_at": 1 })
db.comments.createIndex({ "parent_comment_id": 1 })
```

**ì¿¼ë¦¬ íŒ¨í„´ ë¶„ì„:**
```javascript
// í˜„ì¬ ì§€ì›ë˜ëŠ” ì¿¼ë¦¬
db.comments.find({ "parent_id": ObjectId("...") })               // âœ… ì¸ë±ìŠ¤ ì‚¬ìš©
db.comments.find({ "parent_comment_id": ObjectId("...") })       // âœ… ì¸ë±ìŠ¤ ì‚¬ìš©

// ğŸš¨ ì„±ëŠ¥ ì´ìŠˆ ì¿¼ë¦¬ íŒ¨í„´
db.comments.find({ "author_id": ObjectId("...") })               // âŒ ì¸ë±ìŠ¤ ì—†ìŒ
db.comments.find({ "parent_id": ObjectId("..."), "status": "active" }) // âŒ ë¶€ë¶„ ì¸ë±ìŠ¤ ì‚¬ìš©
db.comments.aggregate([
  { $match: { "parent_id": ObjectId("...") } },
  { $group: { _id: "$metadata.subtype", count: { $sum: 1 } } }
])  // âŒ ì§‘ê³„ ìµœì í™” ë¶€ì¡±
```

**ê°œì„  ë°©ì•ˆ:**
```javascript
// ğŸ”§ ëŒ“ê¸€ ì¡°íšŒ ìµœì í™”
db.comments.createIndex({ "parent_id": 1, "status": 1, "created_at": 1 })
db.comments.createIndex({ "author_id": 1, "created_at": -1 })

// ğŸ”§ ë‹µê¸€ ì‹œìŠ¤í…œ ìµœì í™”
db.comments.createIndex({ "parent_comment_id": 1, "created_at": 1 })

// ğŸ”§ ëŒ“ê¸€ ì§‘ê³„ ìµœì í™”
db.comments.createIndex({ "parent_id": 1, "metadata.subtype": 1, "status": 1 })

// ğŸ”§ ì‚¬ìš©ì í™œë™ ì¶”ì 
db.comments.createIndex({ "author_id": 1, "status": 1, "created_at": -1 })
```

### 1.2 ì¸ë±ìŠ¤ ìµœì í™” êµ¬í˜„

#### ğŸ“ ì¸ë±ìŠ¤ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

```python
# scripts/create_optimized_indexes.py
from motor.motor_asyncio import AsyncIOMotorClient
from nadle_backend.config import settings
import asyncio

class IndexOptimizer:
    def __init__(self):
        self.client = AsyncIOMotorClient(settings.mongodb_url)
        self.db = self.client[settings.database_name]
    
    async def create_all_indexes(self):
        """ëª¨ë“  ìµœì í™”ëœ ì¸ë±ìŠ¤ ìƒì„±"""
        try:
            await self.create_user_indexes()
            await self.create_post_indexes()
            await self.create_comment_indexes()
            await self.create_performance_indexes()
            print("âœ… ëª¨ë“  ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    
    async def create_user_indexes(self):
        """ì‚¬ìš©ì ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤ ìƒì„±"""
        user_collection = self.db.users
        
        # ğŸ”§ ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤
        await user_collection.create_index([
            ("display_name", "text"),
            ("user_handle", "text"),
            ("metadata.bio", "text")
        ], name="user_search_index")
        
        # ğŸ”§ ì§€ì—­ ê²€ìƒ‰ ì¸ë±ìŠ¤
        await user_collection.create_index([
            ("metadata.location", 1)
        ], name="user_location_index")
        
        # ğŸ”§ í™œë™ ì¶”ì  ì¸ë±ìŠ¤
        await user_collection.create_index([
            ("last_login", -1),
            ("status", 1)
        ], name="user_activity_index")
        
        # ğŸ”§ ìƒì„±ì¼ ê¸°ë°˜ ì¸ë±ìŠ¤
        await user_collection.create_index([
            ("created_at", -1),
            ("status", 1)
        ], name="user_created_index")
        
        print("âœ… User ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    async def create_post_indexes(self):
        """ê²Œì‹œê¸€ ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤ ìƒì„±"""
        post_collection = self.db.posts
        
        # ğŸ”§ ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì¸ë±ìŠ¤
        await post_collection.create_index([
            ("metadata.type", 1),
            ("status", 1),
            ("created_at", -1)
        ], name="post_metadata_type_index")
        
        await post_collection.create_index([
            ("metadata.category", 1),
            ("created_at", -1)
        ], name="post_metadata_category_index")
        
        # ğŸ”§ íƒœê·¸ ê²€ìƒ‰ ì¸ë±ìŠ¤
        await post_collection.create_index([
            ("metadata.tags", 1)
        ], name="post_tags_index")
        
        # ğŸ”§ ë³µí•© ê²€ìƒ‰ ì¸ë±ìŠ¤
        await post_collection.create_index([
            ("service", 1),
            ("metadata.type", 1),
            ("status", 1),
            ("created_at", -1)
        ], name="post_service_type_index")
        
        # ğŸ”§ ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤
        await post_collection.create_index([
            ("title", "text"),
            ("content", "text"),
            ("metadata.tags", "text")
        ], 
        weights={
            "title": 10,
            "content": 5,
            "metadata.tags": 3
        },
        name="post_search_index")
        
        # ğŸ”§ í†µê³„ ì¸ë±ìŠ¤
        await post_collection.create_index([
            ("view_count", -1),
            ("status", 1)
        ], name="post_popular_index")
        
        await post_collection.create_index([
            ("created_at", -1),
            ("status", 1)
        ], name="post_recent_index")
        
        print("âœ… Post ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    async def create_comment_indexes(self):
        """ëŒ“ê¸€ ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤ ìƒì„±"""
        comment_collection = self.db.comments
        
        # ğŸ”§ ëŒ“ê¸€ ì¡°íšŒ ìµœì í™”
        await comment_collection.create_index([
            ("parent_id", 1),
            ("status", 1),
            ("created_at", 1)
        ], name="comment_parent_status_index")
        
        # ğŸ”§ ì‚¬ìš©ì ëŒ“ê¸€ ì¡°íšŒ
        await comment_collection.create_index([
            ("author_id", 1),
            ("created_at", -1)
        ], name="comment_author_index")
        
        # ğŸ”§ ë‹µê¸€ ì‹œìŠ¤í…œ ìµœì í™”
        await comment_collection.create_index([
            ("parent_comment_id", 1),
            ("created_at", 1)
        ], name="comment_reply_index")
        
        # ğŸ”§ ëŒ“ê¸€ ì§‘ê³„ ìµœì í™”
        await comment_collection.create_index([
            ("parent_id", 1),
            ("metadata.subtype", 1),
            ("status", 1)
        ], name="comment_aggregation_index")
        
        print("âœ… Comment ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    async def create_performance_indexes(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¸ë±ìŠ¤"""
        
        # ğŸ”§ ì‚¬ìš©ì ë°˜ì‘ ì¸ë±ìŠ¤
        if "user_reactions" in await self.db.list_collection_names():
            reaction_collection = self.db.user_reactions
            
            await reaction_collection.create_index([
                ("user_id", 1),
                ("post_id", 1)
            ], unique=True, name="user_reaction_unique_index")
            
            await reaction_collection.create_index([
                ("post_id", 1),
                ("reaction_type", 1)
            ], name="reaction_stats_index")
        
        # ğŸ”§ ì„¸ì…˜ ê´€ë¦¬ ì¸ë±ìŠ¤
        if "user_sessions" in await self.db.list_collection_names():
            session_collection = self.db.user_sessions
            
            await session_collection.create_index([
                ("user_id", 1),
                ("expires_at", 1)
            ], name="session_user_index")
            
            await session_collection.create_index([
                ("expires_at", 1)
            ], name="session_cleanup_index")
        
        print("âœ… Performance ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    async def analyze_index_usage(self):
        """ì¸ë±ìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        collections = ["users", "posts", "comments"]
        
        for collection_name in collections:
            collection = self.db[collection_name]
            
            # ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ
            stats = await collection.aggregate([
                {"$indexStats": {}}
            ]).to_list(None)
            
            print(f"\nğŸ“Š {collection_name} ì¸ë±ìŠ¤ ì‚¬ìš©ëŸ‰:")
            for stat in stats:
                print(f"  - {stat['name']}: {stat['accesses']['ops']} íšŒ ì‚¬ìš©")
    
    async def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.client.close()

# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
async def main():
    optimizer = IndexOptimizer()
    
    try:
        await optimizer.create_all_indexes()
        await optimizer.analyze_index_usage()
    finally:
        await optimizer.close()

if __name__ == "__main__":
    asyncio.run(main())
```

#### ğŸ“ˆ ì¸ë±ìŠ¤ ì„±ëŠ¥ ì¸¡ì •

```python
# scripts/benchmark_indexes.py
import asyncio
import time
from motor.motor_asyncio import AsyncIOMotorClient
from nadle_backend.config import settings

class IndexBenchmark:
    def __init__(self):
        self.client = AsyncIOMotorClient(settings.mongodb_url)
        self.db = self.client[settings.database_name]
    
    async def benchmark_queries(self):
        """ì¿¼ë¦¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        
        # ğŸ”§ ê²Œì‹œê¸€ ê²€ìƒ‰ ë²¤ì¹˜ë§ˆí¬
        await self.benchmark_post_queries()
        
        # ğŸ”§ ëŒ“ê¸€ ì¡°íšŒ ë²¤ì¹˜ë§ˆí¬
        await self.benchmark_comment_queries()
        
        # ğŸ”§ ì‚¬ìš©ì ê²€ìƒ‰ ë²¤ì¹˜ë§ˆí¬
        await self.benchmark_user_queries()
    
    async def benchmark_post_queries(self):
        """ê²Œì‹œê¸€ ì¿¼ë¦¬ ë²¤ì¹˜ë§ˆí¬"""
        post_collection = self.db.posts
        
        queries = [
            {
                "name": "ë©”íƒ€ë°ì´í„° íƒ€ì… í•„í„°",
                "query": {"metadata.type": "question", "status": "active"},
                "sort": [("created_at", -1)]
            },
            {
                "name": "ì¹´í…Œê³ ë¦¬ í•„í„°",
                "query": {"metadata.category": "tech"},
                "sort": [("created_at", -1)]
            },
            {
                "name": "íƒœê·¸ ê²€ìƒ‰",
                "query": {"metadata.tags": {"$in": ["react", "mongodb"]}},
                "sort": [("created_at", -1)]
            },
            {
                "name": "ì „ë¬¸ ê²€ìƒ‰",
                "query": {"$text": {"$search": "react mongodb"}},
                "sort": []
            }
        ]
        
        print("ğŸ“Š ê²Œì‹œê¸€ ì¿¼ë¦¬ ë²¤ì¹˜ë§ˆí¬:")
        for query_info in queries:
            await self.measure_query_performance(
                post_collection,
                query_info["name"],
                query_info["query"],
                query_info["sort"]
            )
    
    async def benchmark_comment_queries(self):
        """ëŒ“ê¸€ ì¿¼ë¦¬ ë²¤ì¹˜ë§ˆí¬"""
        comment_collection = self.db.comments
        
        # ìƒ˜í”Œ ê²Œì‹œê¸€ ID ì¡°íšŒ
        sample_post = await self.db.posts.find_one({"status": "active"})
        if not sample_post:
            print("âŒ ìƒ˜í”Œ ê²Œì‹œê¸€ì´ ì—†ì–´ ëŒ“ê¸€ ë²¤ì¹˜ë§ˆí¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        queries = [
            {
                "name": "ê²Œì‹œê¸€ ëŒ“ê¸€ ì¡°íšŒ",
                "query": {"parent_id": sample_post["_id"], "status": "active"},
                "sort": [("created_at", 1)]
            },
            {
                "name": "ì‚¬ìš©ì ëŒ“ê¸€ ì¡°íšŒ",
                "query": {"author_id": sample_post["author_id"]},
                "sort": [("created_at", -1)]
            }
        ]
        
        print("\nğŸ“Š ëŒ“ê¸€ ì¿¼ë¦¬ ë²¤ì¹˜ë§ˆí¬:")
        for query_info in queries:
            await self.measure_query_performance(
                comment_collection,
                query_info["name"],
                query_info["query"],
                query_info["sort"]
            )
    
    async def benchmark_user_queries(self):
        """ì‚¬ìš©ì ì¿¼ë¦¬ ë²¤ì¹˜ë§ˆí¬"""
        user_collection = self.db.users
        
        queries = [
            {
                "name": "ì‚¬ìš©ì ê²€ìƒ‰",
                "query": {"$text": {"$search": "admin"}},
                "sort": []
            },
            {
                "name": "ì§€ì—­ ê²€ìƒ‰",
                "query": {"metadata.location": "Seoul"},
                "sort": [("created_at", -1)]
            },
            {
                "name": "í™œì„± ì‚¬ìš©ì ì¡°íšŒ",
                "query": {"status": "active"},
                "sort": [("last_login", -1)]
            }
        ]
        
        print("\nğŸ“Š ì‚¬ìš©ì ì¿¼ë¦¬ ë²¤ì¹˜ë§ˆí¬:")
        for query_info in queries:
            await self.measure_query_performance(
                user_collection,
                query_info["name"],
                query_info["query"],
                query_info["sort"]
            )
    
    async def measure_query_performance(self, collection, name, query, sort):
        """ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •"""
        iterations = 10
        total_time = 0
        
        for _ in range(iterations):
            start_time = time.time()
            
            cursor = collection.find(query)
            if sort:
                cursor = cursor.sort(sort)
            
            # ê²°ê³¼ ì¡°íšŒ (limit 100)
            results = await cursor.limit(100).to_list(100)
            
            end_time = time.time()
            total_time += (end_time - start_time)
        
        avg_time = total_time / iterations
        print(f"  - {name}: {avg_time:.4f}ì´ˆ (í‰ê· ), ê²°ê³¼: {len(results)}ê°œ")
        
        # ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ë¶„ì„
        explain_result = await collection.find(query).explain()
        execution_stats = explain_result.get("executionStats", {})
        
        if execution_stats:
            print(f"    ì‹¤í–‰ í†µê³„ - ê²€ì‚¬ëœ ë¬¸ì„œ: {execution_stats.get('totalDocsExamined', 'N/A')}, "
                  f"ë°˜í™˜ëœ ë¬¸ì„œ: {execution_stats.get('totalDocsReturned', 'N/A')}")
    
    async def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.client.close()

# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
async def main():
    benchmark = IndexBenchmark()
    
    try:
        await benchmark.benchmark_queries()
    finally:
        await benchmark.close()

if __name__ == "__main__":
    asyncio.run(main())
```

**ì˜ˆìƒ ì¸ë±ìŠ¤ ìµœì í™” íš¨ê³¼:**
- ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì¿¼ë¦¬: 70-80% ì„±ëŠ¥ í–¥ìƒ
- ì „ë¬¸ ê²€ìƒ‰ ì¿¼ë¦¬: 90-95% ì„±ëŠ¥ í–¥ìƒ
- ëŒ“ê¸€ ì§‘ê³„ ì¿¼ë¦¬: 60-70% ì„±ëŠ¥ í–¥ìƒ
- ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬: 80-90% ì„±ëŠ¥ í–¥ìƒ

## ğŸ” 2. Redis ìºì‹± ì „ëµ ìµœì í™”

### 2.1 í˜„ì¬ ìºì‹± êµ¬ì¡° ë¶„ì„

#### ğŸ“Š ìºì‹œ ì‚¬ìš© íŒ¨í„´

**í˜„ì¬ ìºì‹œ í‚¤ êµ¬ì¡°:**
```python
# í˜„ì¬ ìºì‹œ í‚¤ íŒ¨í„´
user_cache_key = f"user:{user_id}"
post_cache_key = f"post_detail:{slug_or_id}"
popular_posts_key = f"popular_posts:{category}"
```

**ë¬¸ì œì :**
- ë‹¨ìˆœí•œ í‚¤ êµ¬ì¡°ë¡œ ì¸í•œ ì¶©ëŒ ê°€ëŠ¥ì„±
- ë²„ì „ ê´€ë¦¬ ì—†ìŒ
- ê³„ì¸µì  êµ¬ì¡° ë¶€ì¡±
- ì‚¬ìš©ìë³„ ê°œì¸í™” ì •ë³´ êµ¬ë¶„ ì—†ìŒ

#### ğŸ”§ ê°œì„ ëœ ìºì‹œ í‚¤ ì „ëµ

```python
# nadle_backend/services/enhanced_cache_service.py
from typing import Dict, List, Optional, Any, Union
import hashlib
import json
import time
from enum import Enum

class CacheType(Enum):
    """ìºì‹œ íƒ€ì… ë¶„ë¥˜"""
    USER_PROFILE = "user_profile"
    POST_DETAIL = "post_detail"
    POST_LIST = "post_list"
    COMMENT_LIST = "comment_list"
    POPULAR_CONTENT = "popular_content"
    SEARCH_RESULTS = "search_results"
    USER_ACTIVITY = "user_activity"
    STATISTICS = "statistics"

class CacheTier(Enum):
    """ìºì‹œ ê³„ì¸µ ë¶„ë¥˜"""
    HOT = "hot"      # 5ë¶„ - ì‹¤ì‹œê°„ ë°ì´í„°
    WARM = "warm"    # 30ë¶„ - ìì£¼ ì ‘ê·¼ë˜ëŠ” ë°ì´í„°
    COLD = "cold"    # 1ì‹œê°„ - ë³´í†µ ë°ì´í„°
    FROZEN = "frozen" # 24ì‹œê°„ - ì •ì  ë°ì´í„°

class EnhancedCacheKeyManager:
    """í–¥ìƒëœ ìºì‹œ í‚¤ ê´€ë¦¬"""
    
    def __init__(self, version: str = "v1"):
        self.version = version
        self.key_patterns = {
            CacheType.USER_PROFILE: "{version}:user:profile:{user_id}",
            CacheType.POST_DETAIL: "{version}:post:detail:{post_id}:{user_context}",
            CacheType.POST_LIST: "{version}:post:list:{service}:{filters_hash}:{page}",
            CacheType.COMMENT_LIST: "{version}:comment:list:{post_id}:{sort_type}",
            CacheType.POPULAR_CONTENT: "{version}:popular:{content_type}:{category}:{timeframe}",
            CacheType.SEARCH_RESULTS: "{version}:search:{query_hash}:{filters_hash}:{page}",
            CacheType.USER_ACTIVITY: "{version}:user:activity:{user_id}:{date}",
            CacheType.STATISTICS: "{version}:stats:{metric_type}:{period}",
        }
    
    def generate_key(self, cache_type: CacheType, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        pattern = self.key_patterns.get(cache_type)
        if not pattern:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ìºì‹œ íƒ€ì…: {cache_type}")
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸
        required_params = self._extract_required_params(pattern)
        missing_params = set(required_params) - set(kwargs.keys())
        if missing_params:
            raise ValueError(f"í•„ìˆ˜ íŒŒë¼ë¯¸í„° ëˆ„ë½: {missing_params}")
        
        # í‚¤ ìƒì„±
        return pattern.format(version=self.version, **kwargs)
    
    def _extract_required_params(self, pattern: str) -> List[str]:
        """íŒ¨í„´ì—ì„œ í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
        import re
        return re.findall(r'\{([^}]+)\}', pattern)
    
    def generate_post_detail_key(self, post_id: str, user_id: Optional[str] = None) -> str:
        """ê²Œì‹œê¸€ ìƒì„¸ ìºì‹œ í‚¤ ìƒì„±"""
        user_context = f"user:{user_id}" if user_id else "anonymous"
        return self.generate_key(
            CacheType.POST_DETAIL,
            post_id=post_id,
            user_context=user_context
        )
    
    def generate_post_list_key(self, service: str, filters: Dict[str, Any], page: int = 1) -> str:
        """ê²Œì‹œê¸€ ëª©ë¡ ìºì‹œ í‚¤ ìƒì„±"""
        filters_hash = self._hash_filters(filters)
        return self.generate_key(
            CacheType.POST_LIST,
            service=service,
            filters_hash=filters_hash,
            page=page
        )
    
    def generate_search_key(self, query: str, filters: Dict[str, Any], page: int = 1) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ í‚¤ ìƒì„±"""
        query_hash = self._hash_string(query)
        filters_hash = self._hash_filters(filters)
        return self.generate_key(
            CacheType.SEARCH_RESULTS,
            query_hash=query_hash,
            filters_hash=filters_hash,
            page=page
        )
    
    def _hash_filters(self, filters: Dict[str, Any]) -> str:
        """í•„í„° í•´ì‹œ ìƒì„±"""
        filter_str = json.dumps(filters, sort_keys=True)
        return hashlib.md5(filter_str.encode()).hexdigest()[:8]
    
    def _hash_string(self, text: str) -> str:
        """ë¬¸ìì—´ í•´ì‹œ ìƒì„±"""
        return hashlib.md5(text.encode()).hexdigest()[:8]
    
    def get_related_patterns(self, cache_type: CacheType, **kwargs) -> List[str]:
        """ê´€ë ¨ ìºì‹œ í‚¤ íŒ¨í„´ ë°˜í™˜"""
        if cache_type == CacheType.POST_DETAIL:
            post_id = kwargs.get("post_id")
            return [
                f"{self.version}:post:detail:{post_id}:*",
                f"{self.version}:post:list:*",
                f"{self.version}:popular:*",
                f"{self.version}:search:*"
            ]
        elif cache_type == CacheType.USER_PROFILE:
            user_id = kwargs.get("user_id")
            return [
                f"{self.version}:user:profile:{user_id}",
                f"{self.version}:user:activity:{user_id}:*",
                f"{self.version}:post:list:*",
                f"{self.version}:comment:list:*"
            ]
        else:
            return []

class SmartCacheService:
    """ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì„œë¹„ìŠ¤"""
    
    def __init__(self, redis_manager):
        self.redis = redis_manager
        self.key_manager = EnhancedCacheKeyManager()
        self.cache_tiers = {
            CacheTier.HOT: 300,      # 5ë¶„
            CacheTier.WARM: 1800,    # 30ë¶„
            CacheTier.COLD: 3600,    # 1ì‹œê°„
            CacheTier.FROZEN: 86400, # 24ì‹œê°„
        }
        self.access_tracker = {}
    
    async def get_with_stats(self, cache_type: CacheType, **kwargs) -> Optional[Dict[str, Any]]:
        """í†µê³„ ìˆ˜ì§‘ê³¼ í•¨ê»˜ ìºì‹œ ì¡°íšŒ"""
        cache_key = self.key_manager.generate_key(cache_type, **kwargs)
        
        # ìºì‹œ ì¡°íšŒ
        cached_data = await self.redis.get(cache_key)
        
        if cached_data:
            # ìºì‹œ íˆíŠ¸ í†µê³„ ê¸°ë¡
            await self._record_cache_hit(cache_key)
            return cached_data
        else:
            # ìºì‹œ ë¯¸ìŠ¤ í†µê³„ ê¸°ë¡
            await self._record_cache_miss(cache_key)
            return None
    
    async def set_with_adaptive_ttl(self, cache_type: CacheType, data: Any, **kwargs) -> bool:
        """ì ì‘í˜• TTLë¡œ ìºì‹œ ì„¤ì •"""
        cache_key = self.key_manager.generate_key(cache_type, **kwargs)
        
        # ì ‘ê·¼ ë¹ˆë„ ê¸°ë°˜ TTL ê³„ì‚°
        access_frequency = await self._get_access_frequency(cache_key)
        ttl = await self._calculate_adaptive_ttl(cache_type, access_frequency, data)
        
        # ìºì‹œ ì„¤ì •
        success = await self.redis.set(cache_key, data, ttl)
        
        if success:
            await self._record_cache_set(cache_key, ttl)
        
        return success
    
    async def invalidate_related_cache(self, cache_type: CacheType, **kwargs) -> int:
        """ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”"""
        patterns = self.key_manager.get_related_patterns(cache_type, **kwargs)
        
        invalidated_count = 0
        for pattern in patterns:
            keys = await self.redis.keys(pattern)
            if keys:
                await self.redis.delete(*keys)
                invalidated_count += len(keys)
        
        return invalidated_count
    
    async def _calculate_adaptive_ttl(self, cache_type: CacheType, access_frequency: int, data: Any) -> int:
        """ì ì‘í˜• TTL ê³„ì‚°"""
        
        # ê¸°ë³¸ TTL ê²°ì •
        if cache_type in [CacheType.USER_ACTIVITY, CacheType.STATISTICS]:
            base_tier = CacheTier.HOT
        elif cache_type in [CacheType.POST_DETAIL, CacheType.COMMENT_LIST]:
            base_tier = CacheTier.WARM
        elif cache_type in [CacheType.POST_LIST, CacheType.SEARCH_RESULTS]:
            base_tier = CacheTier.COLD
        else:
            base_tier = CacheTier.FROZEN
        
        base_ttl = self.cache_tiers[base_tier]
        
        # ì ‘ê·¼ ë¹ˆë„ì— ë”°ë¥¸ ì¡°ì •
        if access_frequency > 100:  # ê³ ë¹ˆë„ ì ‘ê·¼
            return max(base_ttl // 2, self.cache_tiers[CacheTier.HOT])
        elif access_frequency > 50:  # ì¤‘ë¹ˆë„ ì ‘ê·¼
            return base_ttl
        else:  # ì €ë¹ˆë„ ì ‘ê·¼
            return min(base_ttl * 2, self.cache_tiers[CacheTier.FROZEN])
    
    async def _get_access_frequency(self, cache_key: str) -> int:
        """ìºì‹œ í‚¤ì˜ ì ‘ê·¼ ë¹ˆë„ ì¡°íšŒ"""
        stats_key = f"cache_stats:access:{cache_key}"
        frequency = await self.redis.get(stats_key)
        return int(frequency) if frequency else 0
    
    async def _record_cache_hit(self, cache_key: str):
        """ìºì‹œ íˆíŠ¸ ê¸°ë¡"""
        stats_key = f"cache_stats:hit:{cache_key}"
        await self.redis.incr(stats_key)
        await self.redis.expire(stats_key, 86400)  # 24ì‹œê°„ ìœ ì§€
    
    async def _record_cache_miss(self, cache_key: str):
        """ìºì‹œ ë¯¸ìŠ¤ ê¸°ë¡"""
        stats_key = f"cache_stats:miss:{cache_key}"
        await self.redis.incr(stats_key)
        await self.redis.expire(stats_key, 86400)  # 24ì‹œê°„ ìœ ì§€
    
    async def _record_cache_set(self, cache_key: str, ttl: int):
        """ìºì‹œ ì„¤ì • ê¸°ë¡"""
        stats_key = f"cache_stats:set:{cache_key}"
        stats_data = {
            "ttl": ttl,
            "timestamp": time.time()
        }
        await self.redis.set(stats_key, stats_data)
        await self.redis.expire(stats_key, 86400)  # 24ì‹œê°„ ìœ ì§€
    
    async def get_cache_statistics(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        
        # ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ í†µê³„
        hit_keys = await self.redis.keys("cache_stats:hit:*")
        miss_keys = await self.redis.keys("cache_stats:miss:*")
        
        total_hits = 0
        total_misses = 0
        
        if hit_keys:
            hit_values = await self.redis.mget(hit_keys)
            total_hits = sum(int(v) for v in hit_values if v)
        
        if miss_keys:
            miss_values = await self.redis.mget(miss_keys)
            total_misses = sum(int(v) for v in miss_values if v)
        
        total_requests = total_hits + total_misses
        hit_rate = (total_hits / total_requests * 100) if total_requests > 0 else 0
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_info = await self.redis.info("memory")
        
        return {
            "hit_rate": round(hit_rate, 2),
            "total_hits": total_hits,
            "total_misses": total_misses,
            "total_requests": total_requests,
            "memory_used": memory_info.get("used_memory_human", "N/A"),
            "memory_peak": memory_info.get("used_memory_peak_human", "N/A"),
            "cache_keys_count": len(await self.redis.keys("v1:*")),
        }
```

### 2.2 Redis ë©”ëª¨ë¦¬ ìµœì í™”

#### ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„

```python
# nadle_backend/services/redis_memory_optimizer.py
import asyncio
from typing import Dict, List, Tuple
from datetime import datetime, timedelta

class RedisMemoryOptimizer:
    """Redis ë©”ëª¨ë¦¬ ìµœì í™” ê´€ë¦¬"""
    
    def __init__(self, redis_manager):
        self.redis = redis_manager
        self.memory_threshold = 80  # 80% ì´ˆê³¼ ì‹œ ì •ë¦¬
        self.cleanup_batch_size = 100
    
    async def optimize_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        
        # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        memory_info = await self.redis.info("memory")
        memory_usage = self._calculate_memory_usage(memory_info)
        
        optimization_result = {
            "before_optimization": memory_usage,
            "actions_taken": [],
            "keys_cleaned": 0,
            "memory_freed": 0
        }
        
        if memory_usage["percentage"] > self.memory_threshold:
            # 1. ë§Œë£Œëœ í‚¤ ì •ë¦¬
            expired_keys = await self._cleanup_expired_keys()
            optimization_result["keys_cleaned"] += expired_keys
            optimization_result["actions_taken"].append(f"ë§Œë£Œëœ í‚¤ {expired_keys}ê°œ ì •ë¦¬")
            
            # 2. ì˜¤ë˜ëœ í†µê³„ ë°ì´í„° ì •ë¦¬
            stats_keys = await self._cleanup_old_stats()
            optimization_result["keys_cleaned"] += stats_keys
            optimization_result["actions_taken"].append(f"ì˜¤ë˜ëœ í†µê³„ {stats_keys}ê°œ ì •ë¦¬")
            
            # 3. LRU ê¸°ë°˜ ì •ë¦¬
            lru_keys = await self._cleanup_lru_keys()
            optimization_result["keys_cleaned"] += lru_keys
            optimization_result["actions_taken"].append(f"LRU í‚¤ {lru_keys}ê°œ ì •ë¦¬")
            
            # ìµœì í™” í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            memory_info_after = await self.redis.info("memory")
            memory_usage_after = self._calculate_memory_usage(memory_info_after)
            
            optimization_result["after_optimization"] = memory_usage_after
            optimization_result["memory_freed"] = memory_usage["used_mb"] - memory_usage_after["used_mb"]
        
        return optimization_result
    
    def _calculate_memory_usage(self, memory_info: Dict) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        used_memory = memory_info.get("used_memory", 0)
        max_memory = memory_info.get("maxmemory", 0)
        
        if max_memory == 0:
            percentage = 0
        else:
            percentage = (used_memory / max_memory) * 100
        
        return {
            "used_mb": round(used_memory / 1024 / 1024, 2),
            "max_mb": round(max_memory / 1024 / 1024, 2),
            "percentage": round(percentage, 2),
            "available_mb": round((max_memory - used_memory) / 1024 / 1024, 2) if max_memory > 0 else 0
        }
    
    async def _cleanup_expired_keys(self) -> int:
        """ë§Œë£Œëœ í‚¤ ì •ë¦¬"""
        cleaned_keys = 0
        
        # ë§Œë£Œëœ í†µê³„ í‚¤ë“¤ ì°¾ê¸°
        stat_patterns = [
            "cache_stats:hit:*",
            "cache_stats:miss:*",
            "cache_stats:set:*"
        ]
        
        for pattern in stat_patterns:
            keys = await self.redis.keys(pattern)
            
            # ë°°ì¹˜ ì²˜ë¦¬
            for i in range(0, len(keys), self.cleanup_batch_size):
                batch_keys = keys[i:i + self.cleanup_batch_size]
                
                # TTL í™•ì¸í•˜ì—¬ ë§Œë£Œëœ í‚¤ ì°¾ê¸°
                for key in batch_keys:
                    ttl = await self.redis.ttl(key)
                    if ttl == -1:  # TTLì´ ì„¤ì •ë˜ì§€ ì•Šì€ í‚¤
                        await self.redis.delete(key)
                        cleaned_keys += 1
        
        return cleaned_keys
    
    async def _cleanup_old_stats(self) -> int:
        """ì˜¤ë˜ëœ í†µê³„ ë°ì´í„° ì •ë¦¬"""
        cleaned_keys = 0
        cutoff_time = datetime.now() - timedelta(days=7)  # 7ì¼ ì´ìƒ ëœ í†µê³„
        
        # ì˜¤ë˜ëœ í†µê³„ í‚¤ ì°¾ê¸°
        stats_keys = await self.redis.keys("cache_stats:*")
        
        for key in stats_keys:
            try:
                # í‚¤ì˜ ìƒì„± ì‹œê°„ í™•ì¸
                key_info = await self.redis.object("idletime", key)
                if key_info and key_info > 604800:  # 7ì¼ (ì´ˆ)
                    await self.redis.delete(key)
                    cleaned_keys += 1
            except:
                continue
        
        return cleaned_keys
    
    async def _cleanup_lru_keys(self) -> int:
        """LRU ê¸°ë°˜ í‚¤ ì •ë¦¬"""
        cleaned_keys = 0
        
        # ì ‘ê·¼ ë¹ˆë„ê°€ ë‚®ì€ ìºì‹œ í‚¤ ì°¾ê¸°
        cache_keys = await self.redis.keys("v1:*")
        
        # í‚¤ë³„ ì ‘ê·¼ ë¹ˆë„ í™•ì¸
        key_access_info = []
        for key in cache_keys:
            try:
                idle_time = await self.redis.object("idletime", key)
                if idle_time and idle_time > 3600:  # 1ì‹œê°„ ì´ìƒ ë¯¸ì‚¬ìš©
                    key_access_info.append((key, idle_time))
            except:
                continue
        
        # ì ‘ê·¼ ë¹ˆë„ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        key_access_info.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ 10% ì •ë¦¬
        cleanup_count = max(1, len(key_access_info) // 10)
        keys_to_cleanup = key_access_info[:cleanup_count]
        
        for key, _ in keys_to_cleanup:
            await self.redis.delete(key)
            cleaned_keys += 1
        
        return cleaned_keys
    
    async def get_memory_report(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³´ê³ ì„œ"""
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory_info = await self.redis.info("memory")
        memory_usage = self._calculate_memory_usage(memory_info)
        
        # í‚¤ ìœ í˜•ë³„ ë¶„ì„
        key_analysis = await self._analyze_key_types()
        
        # ìƒìœ„ ë©”ëª¨ë¦¬ ì‚¬ìš© í‚¤
        top_keys = await self._get_top_memory_keys()
        
        return {
            "memory_usage": memory_usage,
            "key_analysis": key_analysis,
            "top_memory_keys": top_keys,
            "recommendations": self._generate_recommendations(memory_usage, key_analysis)
        }
    
    async def _analyze_key_types(self) -> Dict[str, Any]:
        """í‚¤ ìœ í˜•ë³„ ë¶„ì„"""
        
        key_patterns = {
            "user_profile": "v1:user:profile:*",
            "post_detail": "v1:post:detail:*",
            "post_list": "v1:post:list:*",
            "cache_stats": "cache_stats:*",
            "popular_content": "v1:popular:*",
            "search_results": "v1:search:*"
        }
        
        analysis = {}
        
        for key_type, pattern in key_patterns.items():
            keys = await self.redis.keys(pattern)
            
            total_memory = 0
            for key in keys[:10]:  # ìƒ˜í”Œë§
                try:
                    memory_usage = await self.redis.memory_usage(key)
                    total_memory += memory_usage or 0
                except:
                    continue
            
            avg_memory = total_memory / len(keys) if keys else 0
            estimated_total = avg_memory * len(keys)
            
            analysis[key_type] = {
                "count": len(keys),
                "avg_memory_bytes": round(avg_memory, 2),
                "estimated_total_mb": round(estimated_total / 1024 / 1024, 2)
            }
        
        return analysis
    
    async def _get_top_memory_keys(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìƒìœ„ í‚¤"""
        
        all_keys = await self.redis.keys("*")
        key_memory_info = []
        
        # ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ì€ í‚¤ê°€ ìˆì„ ê²½ìš°)
        sample_keys = all_keys[:1000] if len(all_keys) > 1000 else all_keys
        
        for key in sample_keys:
            try:
                memory_usage = await self.redis.memory_usage(key)
                if memory_usage:
                    key_memory_info.append({
                        "key": key,
                        "memory_bytes": memory_usage,
                        "memory_mb": round(memory_usage / 1024 / 1024, 4)
                    })
            except:
                continue
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        key_memory_info.sort(key=lambda x: x["memory_bytes"], reverse=True)
        
        return key_memory_info[:limit]
    
    def _generate_recommendations(self, memory_usage: Dict, key_analysis: Dict) -> List[str]:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if memory_usage["percentage"] > 80:
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 80%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì •ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if memory_usage["percentage"] > 60:
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ì •ê¸°ì ì¸ ì •ë¦¬ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        # í‚¤ ìœ í˜•ë³„ ê¶Œì¥ì‚¬í•­
        for key_type, analysis in key_analysis.items():
            if analysis["count"] > 10000:
                recommendations.append(f"{key_type} í‚¤ê°€ {analysis['count']}ê°œë¡œ ë§¤ìš° ë§ìŠµë‹ˆë‹¤. TTL ì„¤ì •ì„ ê²€í† í•´ì£¼ì„¸ìš”.")
            
            if analysis["estimated_total_mb"] > 100:
                recommendations.append(f"{key_type} í‚¤ë“¤ì´ {analysis['estimated_total_mb']}MBë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë°ì´í„° êµ¬ì¡°ë¥¼ ìµœì í™”í•´ì£¼ì„¸ìš”.")
        
        return recommendations
```

### 2.3 ìºì‹œ ì›Œë° ì „ëµ

#### ğŸ”§ ì¸ê¸° ì½˜í…ì¸  í”„ë¦¬ë¡œë”©

```python
# nadle_backend/services/cache_warming_service.py
import asyncio
from typing import List, Dict, Any
from datetime import datetime, timedelta

class CacheWarmingService:
    """ìºì‹œ ì›Œë° ì„œë¹„ìŠ¤"""
    
    def __init__(self, redis_manager, post_service, comment_service):
        self.redis = redis_manager
        self.post_service = post_service
        self.comment_service = comment_service
        self.cache_service = SmartCacheService(redis_manager)
    
    async def warm_popular_content(self) -> Dict[str, Any]:
        """ì¸ê¸° ì½˜í…ì¸  ìºì‹œ ì›Œë°"""
        
        warming_result = {
            "posts_warmed": 0,
            "comments_warmed": 0,
            "search_results_warmed": 0,
            "total_time": 0
        }
        
        start_time = datetime.now()
        
        try:
            # 1. ì¸ê¸° ê²Œì‹œê¸€ ìºì‹œ ì›Œë°
            popular_posts = await self._warm_popular_posts()
            warming_result["posts_warmed"] = popular_posts
            
            # 2. ìµœê·¼ ëŒ“ê¸€ ìºì‹œ ì›Œë°
            recent_comments = await self._warm_recent_comments()
            warming_result["comments_warmed"] = recent_comments
            
            # 3. ì¸ê¸° ê²€ìƒ‰ì–´ ê²°ê³¼ ìºì‹œ ì›Œë°
            search_results = await self._warm_popular_searches()
            warming_result["search_results_warmed"] = search_results
            
            # 4. ì‚¬ìš©ì í”„ë¡œí•„ ìºì‹œ ì›Œë°
            user_profiles = await self._warm_active_user_profiles()
            warming_result["user_profiles_warmed"] = user_profiles
            
        except Exception as e:
            warming_result["error"] = str(e)
        
        end_time = datetime.now()
        warming_result["total_time"] = (end_time - start_time).total_seconds()
        
        return warming_result
    
    async def _warm_popular_posts(self) -> int:
        """ì¸ê¸° ê²Œì‹œê¸€ ìºì‹œ ì›Œë°"""
        
        # ìµœê·¼ 7ì¼ê°„ ì¸ê¸° ê²Œì‹œê¸€ ì¡°íšŒ
        popular_posts = await self.post_service.get_popular_posts(
            days=7,
            limit=50
        )
        
        warmed_count = 0
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìºì‹œ ì›Œë°
        warming_tasks = []
        for post in popular_posts:
            task = self._warm_single_post(post["id"])
            warming_tasks.append(task)
            
            # ë°°ì¹˜ í¬ê¸° ì œí•œ
            if len(warming_tasks) >= 10:
                await asyncio.gather(*warming_tasks)
                warmed_count += len(warming_tasks)
                warming_tasks = []
        
        # ë‚¨ì€ íƒœìŠ¤í¬ ì²˜ë¦¬
        if warming_tasks:
            await asyncio.gather(*warming_tasks)
            warmed_count += len(warming_tasks)
        
        return warmed_count
    
    async def _warm_single_post(self, post_id: str):
        """ë‹¨ì¼ ê²Œì‹œê¸€ ìºì‹œ ì›Œë°"""
        
        try:
            # ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ì¡°íšŒ ë° ìºì‹œ
            post_detail = await self.post_service.get_post_detail(post_id)
            
            if post_detail:
                # ìµëª… ì‚¬ìš©ì ë²„ì „ ìºì‹œ
                await self.cache_service.set_with_adaptive_ttl(
                    CacheType.POST_DETAIL,
                    post_detail,
                    post_id=post_id
                )
                
                # ëŒ“ê¸€ ëª©ë¡ ìºì‹œ
                comments = await self.comment_service.get_comments_by_post_id(post_id)
                await self.cache_service.set_with_adaptive_ttl(
                    CacheType.COMMENT_LIST,
                    comments,
                    post_id=post_id,
                    sort_type="recent"
                )
        
        except Exception as e:
            print(f"ê²Œì‹œê¸€ {post_id} ìºì‹œ ì›Œë° ì‹¤íŒ¨: {e}")
    
    async def _warm_recent_comments(self) -> int:
        """ìµœê·¼ ëŒ“ê¸€ ìºì‹œ ì›Œë°"""
        
        # ìµœê·¼ í™œì„± ê²Œì‹œê¸€ ì¡°íšŒ
        recent_posts = await self.post_service.get_recent_posts(
            days=3,
            limit=30
        )
        
        warmed_count = 0
        
        for post in recent_posts:
            try:
                comments = await self.comment_service.get_comments_by_post_id(post["id"])
                
                if comments:
                    await self.cache_service.set_with_adaptive_ttl(
                        CacheType.COMMENT_LIST,
                        comments,
                        post_id=post["id"],
                        sort_type="recent"
                    )
                    warmed_count += 1
            
            except Exception as e:
                print(f"ëŒ“ê¸€ ìºì‹œ ì›Œë° ì‹¤íŒ¨ (ê²Œì‹œê¸€ {post['id']}): {e}")
        
        return warmed_count
    
    async def _warm_popular_searches(self) -> int:
        """ì¸ê¸° ê²€ìƒ‰ì–´ ê²°ê³¼ ìºì‹œ ì›Œë°"""
        
        # ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ (ì˜ˆ: ìµœê·¼ ê²€ìƒ‰ ë¡œê·¸ì—ì„œ)
        popular_queries = await self._get_popular_search_queries()
        
        warmed_count = 0
        
        for query in popular_queries:
            try:
                # ê²€ìƒ‰ ê²°ê³¼ ì¡°íšŒ ë° ìºì‹œ
                search_results = await self.post_service.search_posts(
                    query["term"],
                    filters={}
                )
                
                await self.cache_service.set_with_adaptive_ttl(
                    CacheType.SEARCH_RESULTS,
                    search_results,
                    query_hash=self.cache_service.key_manager._hash_string(query["term"]),
                    filters_hash=self.cache_service.key_manager._hash_filters({}),
                    page=1
                )
                
                warmed_count += 1
            
            except Exception as e:
                print(f"ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì›Œë° ì‹¤íŒ¨ ('{query['term']}'): {e}")
        
        return warmed_count
    
    async def _warm_active_user_profiles(self) -> int:
        """í™œì„± ì‚¬ìš©ì í”„ë¡œí•„ ìºì‹œ ì›Œë°"""
        
        # ìµœê·¼ í™œì„± ì‚¬ìš©ì ì¡°íšŒ
        active_users = await self._get_active_users(days=7, limit=100)
        
        warmed_count = 0
        
        for user in active_users:
            try:
                # ì‚¬ìš©ì í”„ë¡œí•„ ìºì‹œ
                await self.cache_service.set_with_adaptive_ttl(
                    CacheType.USER_PROFILE,
                    user,
                    user_id=user["id"]
                )
                
                warmed_count += 1
            
            except Exception as e:
                print(f"ì‚¬ìš©ì í”„ë¡œí•„ ìºì‹œ ì›Œë° ì‹¤íŒ¨ (ì‚¬ìš©ì {user['id']}): {e}")
        
        return warmed_count
    
    async def _get_popular_search_queries(self, limit: int = 20) -> List[Dict[str, Any]]:
        """ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ"""
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê²€ìƒ‰ ë¡œê·¸ë‚˜ ë¶„ì„ ë°ì´í„°ë¥¼ ì‚¬ìš©
        default_queries = [
            {"term": "react", "frequency": 150},
            {"term": "mongodb", "frequency": 120},
            {"term": "python", "frequency": 100},
            {"term": "javascript", "frequency": 90},
            {"term": "fastapi", "frequency": 80},
            {"term": "typescript", "frequency": 70},
            {"term": "nodejs", "frequency": 60},
            {"term": "docker", "frequency": 50},
            {"term": "aws", "frequency": 45},
            {"term": "kubernetes", "frequency": 40},
        ]
        
        return default_queries[:limit]
    
    async def _get_active_users(self, days: int = 7, limit: int = 100) -> List[Dict[str, Any]]:
        """í™œì„± ì‚¬ìš©ì ì¡°íšŒ"""
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‚¬ìš©ì í™œë™ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¡°íšŒ
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ìš© ë°ì´í„° ë°˜í™˜
        
        return []  # ì‹¤ì œ êµ¬í˜„ í•„ìš”
    
    async def schedule_warming_tasks(self):
        """ìºì‹œ ì›Œë° íƒœìŠ¤í¬ ìŠ¤ì¼€ì¤„ë§"""
        
        # ë§¤ì¼ ìƒˆë²½ 2ì‹œì— ìºì‹œ ì›Œë° ì‹¤í–‰
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” cron jobì´ë‚˜ celery beat ì‚¬ìš©
        
        while True:
            try:
                current_time = datetime.now()
                
                # ìƒˆë²½ 2ì‹œ í™•ì¸
                if current_time.hour == 2 and current_time.minute == 0:
                    print("ìºì‹œ ì›Œë° ì‹œì‘...")
                    result = await self.warm_popular_content()
                    print(f"ìºì‹œ ì›Œë° ì™„ë£Œ: {result}")
                
                # 1ë¶„ ëŒ€ê¸°
                await asyncio.sleep(60)
                
            except Exception as e:
                print(f"ìºì‹œ ì›Œë° ìŠ¤ì¼€ì¤„ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)
```

**ì˜ˆìƒ ìºì‹œ ìµœì í™” íš¨ê³¼:**
- ìºì‹œ íˆíŠ¸ìœ¨: 75% â†’ 90% í–¥ìƒ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: 40-50% í–¥ìƒ
- ì‘ë‹µ ì‹œê°„: 20-30% ë‹¨ì¶•
- ì‹œìŠ¤í…œ ì•ˆì •ì„±: ëŒ€í­ í–¥ìƒ

## ğŸ” 3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ë§ ìµœì í™”

### 3.1 í˜„ì¬ ì—°ê²° ì„¤ì • ë¶„ì„

#### ğŸ“Š í˜„ì¬ ì—°ê²° ì„¤ì • ë¬¸ì œì 

**í˜„ì¬ ì„¤ì •:** `nadle_backend/database/connection.py`
```python
# í˜„ì¬ ë¬¸ì œì ì´ ìˆëŠ” ì„¤ì •
self.client = AsyncIOMotorClient(
    settings.mongodb_url,
    serverSelectionTimeoutMS=5000  # ë„ˆë¬´ ì§§ìŒ
    # ğŸš¨ maxPoolSize ì„¤ì • ì—†ìŒ
    # ğŸš¨ minPoolSize ì„¤ì • ì—†ìŒ
    # ğŸš¨ maxIdleTimeMS ì„¤ì • ì—†ìŒ
    # ğŸš¨ waitQueueTimeoutMS ì„¤ì • ì—†ìŒ
)
```

#### ğŸ”§ ìµœì í™”ëœ ì—°ê²° ì„¤ì •

```python
# nadle_backend/database/optimized_connection.py
from motor.motor_asyncio import AsyncIOMotorClient
from pymongo import MongoClient
from pymongo.errors import ServerSelectionTimeoutError
import asyncio
import logging
from typing import Optional, Dict, Any
from datetime import datetime

logger = logging.getLogger(__name__)

class OptimizedMongoConnection:
    """ìµœì í™”ëœ MongoDB ì—°ê²° ê´€ë¦¬"""
    
    def __init__(self, mongodb_url: str, database_name: str):
        self.mongodb_url = mongodb_url
        self.database_name = database_name
        self.client: Optional[AsyncIOMotorClient] = None
        self.db = None
        self.connection_stats = {
            "connections_created": 0,
            "connections_closed": 0,
            "connection_failures": 0,
            "last_connection_time": None,
            "last_error": None
        }
    
    async def connect(self) -> bool:
        """ìµœì í™”ëœ ì—°ê²° ì„¤ì •ìœ¼ë¡œ MongoDB ì—°ê²°"""
        
        try:
            # ğŸ”§ ìµœì í™”ëœ ì—°ê²° ì„¤ì •
            self.client = AsyncIOMotorClient(
                self.mongodb_url,
                
                # ğŸ”§ ì—°ê²° í’€ ì„¤ì •
                maxPoolSize=20,           # ìµœëŒ€ ì—°ê²° ìˆ˜
                minPoolSize=5,            # ìµœì†Œ ì—°ê²° ìˆ˜
                maxIdleTimeMS=30000,      # 30ì´ˆ í›„ ìœ íœ´ ì—°ê²° í•´ì œ
                waitQueueTimeoutMS=10000, # 10ì´ˆ ì—°ê²° ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ
                
                # ğŸ”§ ì„œë²„ ì„ íƒ ë° í—¬ìŠ¤ì²´í¬
                serverSelectionTimeoutMS=10000,  # 10ì´ˆ ì„œë²„ ì„ íƒ íƒ€ì„ì•„ì›ƒ
                heartbeatFrequencyMS=10000,       # 10ì´ˆë§ˆë‹¤ í—¬ìŠ¤ì²´í¬
                
                # ğŸ”§ ì—°ê²° ìœ ì§€ ì„¤ì •
                connectTimeoutMS=10000,    # 10ì´ˆ ì—°ê²° íƒ€ì„ì•„ì›ƒ
                socketTimeoutMS=30000,     # 30ì´ˆ ì†Œì¼“ íƒ€ì„ì•„ì›ƒ
                
                # ğŸ”§ ì¬ì‹œë„ ì„¤ì •
                retryWrites=True,          # ì“°ê¸° ì¬ì‹œë„ í™œì„±í™”
                retryReads=True,           # ì½ê¸° ì¬ì‹œë„ í™œì„±í™”
                
                # ğŸ”§ ì••ì¶• ì„¤ì •
                compressors=['zstd', 'zlib', 'snappy'],
                
                # ğŸ”§ ë¡œê¹… ì„¤ì •
                event_listeners=[ConnectionEventListener()],
                
                # ğŸ”§ ì½ê¸° ì„¤ì •
                readPreference='primaryPreferred',
                readConcern={'level': 'local'},
                
                # ğŸ”§ ì“°ê¸° ì„¤ì •
                writeConcern={'w': 1, 'j': True},
                
                # ğŸ”§ SSL ì„¤ì • (í•„ìš”í•œ ê²½ìš°)
                ssl=True if 'ssl=true' in self.mongodb_url else False,
                
                # ğŸ”§ ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ë¦„
                appname='XAI-Community-Backend'
            )
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            await self.client.admin.command('ping')
            
            self.db = self.client[self.database_name]
            
            # ì—°ê²° í†µê³„ ì—…ë°ì´íŠ¸
            self.connection_stats["connections_created"] += 1
            self.connection_stats["last_connection_time"] = datetime.now()
            
            logger.info("MongoDB ì—°ê²° ì„±ê³µ")
            return True
            
        except Exception as e:
            self.connection_stats["connection_failures"] += 1
            self.connection_stats["last_error"] = str(e)
            logger.error(f"MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    async def disconnect(self):
        """ì—°ê²° ì¢…ë£Œ"""
        if self.client:
            self.client.close()
            self.connection_stats["connections_closed"] += 1
            logger.info("MongoDB ì—°ê²° ì¢…ë£Œ")
    
    async def get_connection_stats(self) -> Dict[str, Any]:
        """ì—°ê²° í†µê³„ ì¡°íšŒ"""
        if not self.client:
            return self.connection_stats
        
        try:
            # ì„œë²„ ìƒíƒœ ì •ë³´
            server_status = await self.client.admin.command('serverStatus')
            
            # ì—°ê²° í’€ ìƒíƒœ
            connection_pool_stats = {
                "connections_current": server_status.get('connections', {}).get('current', 0),
                "connections_available": server_status.get('connections', {}).get('available', 0),
                "connections_total_created": server_status.get('connections', {}).get('totalCreated', 0),
            }
            
            # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
            db_stats = await self.db.command('dbstats')
            
            return {
                **self.connection_stats,
                "pool_stats": connection_pool_stats,
                "db_stats": {
                    "collections": db_stats.get('collections', 0),
                    "objects": db_stats.get('objects', 0),
                    "data_size": db_stats.get('dataSize', 0),
                    "storage_size": db_stats.get('storageSize', 0),
                    "indexes": db_stats.get('indexes', 0),
                    "index_size": db_stats.get('indexSize', 0),
                }
            }
            
        except Exception as e:
            logger.error(f"ì—°ê²° í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self.connection_stats
    
    async def health_check(self) -> Dict[str, Any]:
        """ì—°ê²° ìƒíƒœ í—¬ìŠ¤ì²´í¬"""
        
        health_status = {
            "healthy": False,
            "response_time": None,
            "error": None,
            "timestamp": datetime.now().isoformat()
        }
        
        if not self.client:
            health_status["error"] = "í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°ë˜ì§€ ì•ŠìŒ"
            return health_status
        
        try:
            start_time = datetime.now()
            
            # ping í…ŒìŠ¤íŠ¸
            await self.client.admin.command('ping')
            
            end_time = datetime.now()
            response_time = (end_time - start_time).total_seconds() * 1000
            
            health_status["healthy"] = True
            health_status["response_time"] = round(response_time, 2)
            
        except Exception as e:
            health_status["error"] = str(e)
            logger.error(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
        
        return health_status
    
    async def optimize_connection_pool(self):
        """ì—°ê²° í’€ ìµœì í™”"""
        
        if not self.client:
            return
        
        try:
            # í˜„ì¬ ì—°ê²° ìƒíƒœ í™•ì¸
            stats = await self.get_connection_stats()
            pool_stats = stats.get("pool_stats", {})
            
            current_connections = pool_stats.get("connections_current", 0)
            available_connections = pool_stats.get("connections_available", 0)
            
            logger.info(f"í˜„ì¬ ì—°ê²° ìˆ˜: {current_connections}, ì‚¬ìš© ê°€ëŠ¥: {available_connections}")
            
            # ì—°ê²° í’€ ì¡°ì • ë¡œì§ (í•„ìš”ì‹œ)
            utilization = (current_connections - available_connections) / current_connections if current_connections > 0 else 0
            
            if utilization > 0.8:  # 80% ì´ìƒ ì‚¬ìš© ì¤‘
                logger.warning(f"ì—°ê²° í’€ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤: {utilization:.2%}")
                # í•„ìš”ì‹œ ì—°ê²° í’€ í¬ê¸° ì¡°ì • ë˜ëŠ” ì•Œë¦¼
            
        except Exception as e:
            logger.error(f"ì—°ê²° í’€ ìµœì í™” ì‹¤íŒ¨: {e}")

class ConnectionEventListener:
    """ì—°ê²° ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ"""
    
    def started(self, event):
        logger.debug(f"ì—°ê²° ì‹œì‘: {event.connection_id}")
    
    def succeeded(self, event):
        logger.debug(f"ì—°ê²° ì„±ê³µ: {event.connection_id}, ì‹œê°„: {event.duration}ms")
    
    def failed(self, event):
        logger.error(f"ì—°ê²° ì‹¤íŒ¨: {event.connection_id}, ì˜¤ë¥˜: {event.failure}")
    
    def closed(self, event):
        logger.debug(f"ì—°ê²° ì¢…ë£Œ: {event.connection_id}")

class ConnectionPoolManager:
    """ì—°ê²° í’€ ê´€ë¦¬ì"""
    
    def __init__(self, connection: OptimizedMongoConnection):
        self.connection = connection
        self.monitoring_enabled = True
        self.monitoring_interval = 60  # 1ë¶„ë§ˆë‹¤ ëª¨ë‹ˆí„°ë§
    
    async def start_monitoring(self):
        """ì—°ê²° í’€ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        
        while self.monitoring_enabled:
            try:
                # í—¬ìŠ¤ì²´í¬
                health = await self.connection.health_check()
                
                if not health["healthy"]:
                    logger.warning(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ ë¶ˆëŸ‰: {health['error']}")
                    
                    # ì¬ì—°ê²° ì‹œë„
                    await self.connection.disconnect()
                    await asyncio.sleep(5)
                    await self.connection.connect()
                
                # ì—°ê²° í’€ ìµœì í™”
                await self.connection.optimize_connection_pool()
                
                # ëª¨ë‹ˆí„°ë§ ê°„ê²© ëŒ€ê¸°
                await asyncio.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"ì—°ê²° í’€ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(self.monitoring_interval)
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨"""
        self.monitoring_enabled = False
```

### 3.2 ì—°ê²° í’€ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

```python
# nadle_backend/monitoring/connection_monitor.py
import asyncio
import json
from typing import Dict, List, Any
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict

@dataclass
class ConnectionMetrics:
    """ì—°ê²° ë©”íŠ¸ë¦­ ë°ì´í„°"""
    timestamp: datetime
    current_connections: int
    available_connections: int
    total_created: int
    pool_utilization: float
    response_time: float
    errors: int

class ConnectionMonitor:
    """ì—°ê²° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, connection_manager: OptimizedMongoConnection):
        self.connection_manager = connection_manager
        self.metrics_history: List[ConnectionMetrics] = []
        self.max_history_size = 1440  # 24ì‹œê°„ (1ë¶„ë§ˆë‹¤ ê¸°ë¡)
        self.alert_thresholds = {
            "high_utilization": 0.8,
            "slow_response": 1000,  # 1ì´ˆ
            "connection_errors": 5
        }
    
    async def collect_metrics(self) -> ConnectionMetrics:
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        
        # í—¬ìŠ¤ì²´í¬
        health = await self.connection_manager.health_check()
        
        # ì—°ê²° í†µê³„
        stats = await self.connection_manager.get_connection_stats()
        pool_stats = stats.get("pool_stats", {})
        
        current_connections = pool_stats.get("connections_current", 0)
        available_connections = pool_stats.get("connections_available", 0)
        
        # ì‚¬ìš©ë¥  ê³„ì‚°
        utilization = (current_connections - available_connections) / current_connections if current_connections > 0 else 0
        
        metrics = ConnectionMetrics(
            timestamp=datetime.now(),
            current_connections=current_connections,
            available_connections=available_connections,
            total_created=pool_stats.get("connections_total_created", 0),
            pool_utilization=utilization,
            response_time=health.get("response_time", 0),
            errors=stats.get("connection_failures", 0)
        )
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.metrics_history.append(metrics)
        
        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
        if len(self.metrics_history) > self.max_history_size:
            self.metrics_history = self.metrics_history[-self.max_history_size:]
        
        return metrics
    
    async def check_alerts(self, metrics: ConnectionMetrics) -> List[Dict[str, Any]]:
        """ì•Œë¦¼ ì¡°ê±´ í™•ì¸"""
        
        alerts = []
        
        # ë†’ì€ ì‚¬ìš©ë¥  ì•Œë¦¼
        if metrics.pool_utilization > self.alert_thresholds["high_utilization"]:
            alerts.append({
                "type": "high_utilization",
                "severity": "warning",
                "message": f"ì—°ê²° í’€ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤: {metrics.pool_utilization:.2%}",
                "value": metrics.pool_utilization,
                "threshold": self.alert_thresholds["high_utilization"]
            })
        
        # ëŠë¦° ì‘ë‹µ ì•Œë¦¼
        if metrics.response_time > self.alert_thresholds["slow_response"]:
            alerts.append({
                "type": "slow_response",
                "severity": "warning",
                "message": f"ë°ì´í„°ë² ì´ìŠ¤ ì‘ë‹µì´ ëŠë¦½ë‹ˆë‹¤: {metrics.response_time}ms",
                "value": metrics.response_time,
                "threshold": self.alert_thresholds["slow_response"]
            })
        
        # ì—°ê²° ì˜¤ë¥˜ ì•Œë¦¼
        if metrics.errors > self.alert_thresholds["connection_errors"]:
            alerts.append({
                "type": "connection_errors",
                "severity": "error",
                "message": f"ì—°ê²° ì˜¤ë¥˜ê°€ ë¹ˆë²ˆí•©ë‹ˆë‹¤: {metrics.errors}íšŒ",
                "value": metrics.errors,
                "threshold": self.alert_thresholds["connection_errors"]
            })
        
        return alerts
    
    async def generate_report(self, hours: int = 24) -> Dict[str, Any]:
        """ì—°ê²° ìƒíƒœ ë³´ê³ ì„œ ìƒì„±"""
        
        # ì§€ì •ëœ ì‹œê°„ ë‚´ì˜ ë©”íŠ¸ë¦­ í•„í„°ë§
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [m for m in self.metrics_history if m.timestamp >= cutoff_time]
        
        if not recent_metrics:
            return {"error": "ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # í†µê³„ ê³„ì‚°
        avg_utilization = sum(m.pool_utilization for m in recent_metrics) / len(recent_metrics)
        max_utilization = max(m.pool_utilization for m in recent_metrics)
        avg_response_time = sum(m.response_time for m in recent_metrics) / len(recent_metrics)
        max_response_time = max(m.response_time for m in recent_metrics)
        
        total_errors = recent_metrics[-1].errors - recent_metrics[0].errors if len(recent_metrics) > 1 else 0
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        hourly_stats = self._analyze_hourly_patterns(recent_metrics)
        
        return {
            "report_period": f"{hours}ì‹œê°„",
            "total_data_points": len(recent_metrics),
            "summary": {
                "avg_utilization": round(avg_utilization, 4),
                "max_utilization": round(max_utilization, 4),
                "avg_response_time": round(avg_response_time, 2),
                "max_response_time": round(max_response_time, 2),
                "total_errors": total_errors,
                "current_connections": recent_metrics[-1].current_connections,
                "available_connections": recent_metrics[-1].available_connections,
            },
            "hourly_patterns": hourly_stats,
            "recommendations": self._generate_recommendations(recent_metrics)
        }
    
    def _analyze_hourly_patterns(self, metrics: List[ConnectionMetrics]) -> Dict[str, Any]:
        """ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„"""
        
        hourly_data = {}
        
        for metric in metrics:
            hour = metric.timestamp.hour
            
            if hour not in hourly_data:
                hourly_data[hour] = {
                    "utilization": [],
                    "response_times": [],
                    "connections": []
                }
            
            hourly_data[hour]["utilization"].append(metric.pool_utilization)
            hourly_data[hour]["response_times"].append(metric.response_time)
            hourly_data[hour]["connections"].append(metric.current_connections)
        
        # ì‹œê°„ëŒ€ë³„ í‰ê·  ê³„ì‚°
        hourly_stats = {}
        for hour, data in hourly_data.items():
            hourly_stats[hour] = {
                "avg_utilization": round(sum(data["utilization"]) / len(data["utilization"]), 4),
                "avg_response_time": round(sum(data["response_times"]) / len(data["response_times"]), 2),
                "avg_connections": round(sum(data["connections"]) / len(data["connections"]), 1)
            }
        
        return hourly_stats
    
    def _generate_recommendations(self, metrics: List[ConnectionMetrics]) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # í‰ê·  ì‚¬ìš©ë¥  ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        avg_utilization = sum(m.pool_utilization for m in metrics) / len(metrics)
        
        if avg_utilization > 0.7:
            recommendations.append("ì—°ê²° í’€ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        if avg_utilization < 0.3:
            recommendations.append("ì—°ê²° í’€ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ë¦¬ì†ŒìŠ¤ë¥¼ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        avg_response_time = sum(m.response_time for m in metrics) / len(metrics)
        
        if avg_response_time > 500:
            recommendations.append("ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        if avg_response_time > 1000:
            recommendations.append("ì¸ë±ìŠ¤ ìµœì í™” ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤ì¼€ì¼ë§ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        # ì˜¤ë¥˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        error_rate = metrics[-1].errors / len(metrics) if len(metrics) > 0 else 0
        
        if error_rate > 0.1:
            recommendations.append("ì—°ê²° ì˜¤ë¥˜ê°€ ìì£¼ ë°œìƒí•©ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
        
        return recommendations

# ì‚¬ìš© ì˜ˆì‹œ
async def setup_connection_monitoring():
    """ì—°ê²° ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
    
    # ì—°ê²° ê´€ë¦¬ì ìƒì„±
    connection = OptimizedMongoConnection(
        mongodb_url="mongodb://localhost:27017",
        database_name="xai_community"
    )
    
    # ì—°ê²°
    await connection.connect()
    
    # ëª¨ë‹ˆí„° ìƒì„±
    monitor = ConnectionMonitor(connection)
    
    # ì •ê¸°ì ì¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    async def collect_metrics_periodically():
        while True:
            try:
                metrics = await monitor.collect_metrics()
                alerts = await monitor.check_alerts(metrics)
                
                # ì•Œë¦¼ ì²˜ë¦¬
                for alert in alerts:
                    print(f"ğŸš¨ {alert['severity'].upper()}: {alert['message']}")
                
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìˆ˜ì§‘
                
            except Exception as e:
                print(f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)
    
    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘
    asyncio.create_task(collect_metrics_periodically())
    
    return connection, monitor
```

**ì˜ˆìƒ ì—°ê²° ìµœì í™” íš¨ê³¼:**
- ë™ì‹œ ì—°ê²° ì²˜ë¦¬ ëŠ¥ë ¥: 3-5ë°° í–¥ìƒ
- ì—°ê²° ì•ˆì •ì„±: 95% â†’ 99.5% í–¥ìƒ
- ì‘ë‹µ ì‹œê°„ ì¼ê´€ì„±: í‘œì¤€í¸ì°¨ 50% ê°ì†Œ
- ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±: 30-40% í–¥ìƒ

## ğŸ” 4. ì§‘ê³„ íŒŒì´í”„ë¼ì¸ ìµœì í™”

### 4.1 í˜„ì¬ ì§‘ê³„ ì„±ëŠ¥ ë¶„ì„

#### ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„

**ê¸°ì¡´ ì„±ëŠ¥ ë°ì´í„°:**
```json
{
  "ê¸°ì¡´_Full_ì—”ë“œí¬ì¸íŠ¸": {
    "í‰ê· _ì‘ë‹µì‹œê°„": "78.62ms",
    "í‘œì¤€í¸ì°¨": "30.84ms",
    "ì•ˆì •ì„±": "ë¶ˆì•ˆì •"
  },
  "ì™„ì „í†µí•©_Aggregation": {
    "í‰ê· _ì‘ë‹µì‹œê°„": "46.83ms",
    "í‘œì¤€í¸ì°¨": "4.91ms",
    "ì•ˆì •ì„±": "ì•ˆì •ì ",
    "ì„±ëŠ¥_í–¥ìƒ": "40.4%"
  }
}
```

#### ğŸ”§ ì¶”ê°€ ìµœì í™” ë°©ì•ˆ

```python
# nadle_backend/services/optimized_aggregation_service.py
from typing import Dict, List, Any, Optional
from datetime import datetime
import asyncio

class OptimizedAggregationService:
    """ìµœì í™”ëœ ì§‘ê³„ ì„œë¹„ìŠ¤"""
    
    def __init__(self, post_repository, comment_repository, user_repository):
        self.post_repository = post_repository
        self.comment_repository = comment_repository
        self.user_repository = user_repository
    
    async def get_post_with_everything_optimized(self, slug_or_id: str, user_id: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """ìµœì í™”ëœ ê²Œì‹œê¸€ ì „ì²´ ì •ë³´ ì¡°íšŒ"""
        
        # ğŸ”§ 1ë‹¨ê³„: ì¸ë±ìŠ¤ íŒíŠ¸ë¥¼ ì‚¬ìš©í•œ ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸
        optimized_pipeline = self._build_optimized_pipeline(slug_or_id)
        
        # ğŸ”§ 2ë‹¨ê³„: ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì‚¬ìš©ìë³„ ì •ë³´ ì¡°íšŒ
        post_task = self.post_repository.aggregate(optimized_pipeline)
        user_data_task = self._get_user_specific_data(slug_or_id, user_id) if user_id else None
        
        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(
            post_task,
            user_data_task,
            return_exceptions=True
        )
        
        post_result = results[0]
        user_data = results[1] if len(results) > 1 and not isinstance(results[1], Exception) else None
        
        if not post_result:
            return None
        
        post_data = post_result[0]
        
        # ğŸ”§ 3ë‹¨ê³„: ì‚¬ìš©ìë³„ ë°ì´í„° ë³‘í•©
        if user_data:
            post_data['user_reactions'] = user_data.get('reactions', {})
            post_data['user_bookmarked'] = user_data.get('bookmarked', False)
            post_data['user_notifications'] = user_data.get('notifications', {})
        
        return post_data
    
    def _build_optimized_pipeline(self, slug_or_id: str) -> List[Dict[str, Any]]:
        """ìµœì í™”ëœ ì§‘ê³„ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"""
        
        # ğŸ”§ ë§¤ì¹˜ ìŠ¤í…Œì´ì§€ ìµœì í™”
        match_stage = self._build_match_stage(slug_or_id)
        
        return [
            # Stage 1: ì¸ë±ìŠ¤ íŒíŠ¸ì™€ í•¨ê»˜ ë§¤ì¹˜
            {
                "$match": match_stage
            },
            {
                "$hint": {"slug": 1}  # ìŠ¬ëŸ¬ê·¸ ì¸ë±ìŠ¤ íŒíŠ¸
            },
            
            # Stage 2: ì‘ì„±ì ì •ë³´ ì¡°ì¸ (í•„ìš”í•œ í•„ë“œë§Œ)
            {
                "$lookup": {
                    "from": "users",
                    "localField": "author_id",
                    "foreignField": "_id",
                    "as": "author_info",
                    "pipeline": [
                        {
                            "$project": {
                                "_id": 1,
                                "display_name": 1,
                                "user_handle": 1,
                                "avatar_url": 1,
                                "reputation": 1
                            }
                        }
                    ]
                }
            },
            
            # Stage 3: ëŒ“ê¸€ í†µê³„ ì§‘ê³„ (ì „ì²´ ëŒ“ê¸€ ë¡œë“œí•˜ì§€ ì•ŠìŒ)
            {
                "$lookup": {
                    "from": "comments",
                    "localField": "_id",
                    "foreignField": "parent_id",
                    "as": "comment_stats",
                    "pipeline": [
                        {
                            "$match": {
                                "status": "active"
                            }
                        },
                        {
                            "$group": {
                                "_id": None,
                                "total_comments": {"$sum": 1},
                                "recent_comments": {
                                    "$push": {
                                        "$cond": [
                                            {
                                                "$gte": [
                                                    "$created_at",
                                                    {"$subtract": [datetime.utcnow(), 7 * 24 * 60 * 60 * 1000]}
                                                ]
                                            },
                                            {
                                                "_id": "$_id",
                                                "content": {"$substr": ["$content", 0, 100]},
                                                "author_id": "$author_id",
                                                "created_at": "$created_at"
                                            },
                                            None
                                        ]
                                    }
                                }
                            }
                        },
                        {
                            "$project": {
                                "total_comments": 1,
                                "recent_comments": {
                                    "$slice": [
                                        {"$filter": {
                                            "input": "$recent_comments",
                                            "cond": {"$ne": ["$$this", None]}
                                        }},
                                        5
                                    ]
                                }
                            }
                        }
                    ]
                }
            },
            
            # Stage 4: ë°˜ì‘ í†µê³„ ì§‘ê³„
            {
                "$lookup": {
                    "from": "user_reactions",
                    "localField": "_id",
                    "foreignField": "post_id",
                    "as": "reaction_stats",
                    "pipeline": [
                        {
                            "$group": {
                                "_id": "$reaction_type",
                                "count": {"$sum": 1}
                            }
                        }
                    ]
                }
            },
            
            # Stage 5: ìµœì¢… ë°ì´í„° êµ¬ì¡°í™”
            {
                "$addFields": {
                    "author": {"$arrayElemAt": ["$author_info", 0]},
                    "stats": {
                        "$mergeObjects": [
                            {"$arrayElemAt": ["$comment_stats", 0]},
                            {
                                "reactions": {
                                    "$arrayToObject": {
                                        "$map": {
                                            "input": "$reaction_stats",
                                            "as": "reaction",
                                            "in": {
                                                "k": "$$reaction._id",
                                                "v": "$$reaction.count"
                                            }
                                        }
                                    }
                                }
                            }
                        ]
                    }
                }
            },
            
            # Stage 6: ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°
            {
                "$project": {
                    "author_info": 0,
                    "comment_stats": 0,
                    "reaction_stats": 0
                }
            }
        ]
    
    def _build_match_stage(self, slug_or_id: str) -> Dict[str, Any]:
        """ë§¤ì¹˜ ìŠ¤í…Œì´ì§€ êµ¬ì¶•"""
        
        # ObjectId í˜•íƒœì¸ì§€ í™•ì¸
        if self._is_object_id(slug_or_id):
            return {
                "_id": ObjectId(slug_or_id),
                "status": "active"
            }
        else:
            return {
                "slug": slug_or_id,
                "status": "active"
            }
    
    async def _get_user_specific_data(self, post_id: str, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ìë³„ ê°œì¸í™” ë°ì´í„° ì¡°íšŒ"""
        
        # ë³‘ë ¬ë¡œ ì‚¬ìš©ìë³„ ë°ì´í„° ì¡°íšŒ
        tasks = [
            self._get_user_reactions(post_id, user_id),
            self._get_user_bookmark_status(post_id, user_id),
            self._get_user_notification_settings(post_id, user_id)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return {
            "reactions": results[0] if not isinstance(results[0], Exception) else {},
            "bookmarked": results[1] if not isinstance(results[1], Exception) else False,
            "notifications": results[2] if not isinstance(results[2], Exception) else {}
        }
    
    async def _get_user_reactions(self, post_id: str, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ë°˜ì‘ ì¡°íšŒ"""
        
        reaction = await self.user_reaction_repository.find_one({
            "post_id": post_id,
            "user_id": user_id
        })
        
        return {
            "liked": reaction.get("reaction_type") == "like" if reaction else False,
            "disliked": reaction.get("reaction_type") == "dislike" if reaction else False,
            "reaction_date": reaction.get("created_at") if reaction else None
        }
    
    async def _get_user_bookmark_status(self, post_id: str, user_id: str) -> bool:
        """ì‚¬ìš©ì ë¶ë§ˆí¬ ìƒíƒœ ì¡°íšŒ"""
        
        bookmark = await self.user_bookmark_repository.find_one({
            "post_id": post_id,
            "user_id": user_id
        })
        
        return bookmark is not None
    
    async def _get_user_notification_settings(self, post_id: str, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì•Œë¦¼ ì„¤ì • ì¡°íšŒ"""
        
        notification = await self.user_notification_repository.find_one({
            "post_id": post_id,
            "user_id": user_id
        })
        
        return {
            "comment_notifications": notification.get("comment_notifications", False) if notification else False,
            "reaction_notifications": notification.get("reaction_notifications", False) if notification else False
        }
    
    def _is_object_id(self, value: str) -> bool:
        """ObjectId í˜•íƒœì¸ì§€ í™•ì¸"""
        try:
            ObjectId(value)
            return True
        except:
            return False

class BatchAggregationService:
    """ë°°ì¹˜ ì§‘ê³„ ì„œë¹„ìŠ¤"""
    
    def __init__(self, aggregation_service: OptimizedAggregationService):
        self.aggregation_service = aggregation_service
    
    async def get_multiple_posts_optimized(self, post_ids: List[str], user_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ê²Œì‹œê¸€ ìµœì í™”ëœ ì¡°íšŒ"""
        
        # ğŸ”§ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
        batch_size = 10
        results = []
        
        for i in range(0, len(post_ids), batch_size):
            batch_ids = post_ids[i:i + batch_size]
            
            # ë³‘ë ¬ ì²˜ë¦¬
            batch_tasks = [
                self.aggregation_service.get_post_with_everything_optimized(post_id, user_id)
                for post_id in batch_ids
            ]
            
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # ì„±ê³µí•œ ê²°ê³¼ë§Œ ì¶”ê°€
            for result in batch_results:
                if not isinstance(result, Exception) and result:
                    results.append(result)
        
        return results
    
    async def get_popular_posts_with_stats(self, category: str, limit: int = 20) -> List[Dict[str, Any]]:
        """ì¸ê¸° ê²Œì‹œê¸€ í†µê³„ì™€ í•¨ê»˜ ì¡°íšŒ"""
        
        # ğŸ”§ ìµœì í™”ëœ ì¸ê¸° ê²Œì‹œê¸€ ì§‘ê³„
        pipeline = [
            {
                "$match": {
                    "metadata.category": category,
                    "status": "active",
                    "created_at": {
                        "$gte": datetime.utcnow() - timedelta(days=7)
                    }
                }
            },
            {
                "$lookup": {
                    "from": "user_reactions",
                    "localField": "_id",
                    "foreignField": "post_id",
                    "as": "reactions"
                }
            },
            {
                "$lookup": {
                    "from": "comments",
                    "localField": "_id",
                    "foreignField": "parent_id",
                    "as": "comments",
                    "pipeline": [
                        {"$match": {"status": "active"}},
                        {"$count": "total"}
                    ]
                }
            },
            {
                "$addFields": {
                    "popularity_score": {
                        "$add": [
                            {"$multiply": [{"$size": "$reactions"}, 2]},  # ë°˜ì‘ ê°€ì¤‘ì¹˜ 2
                            {"$multiply": [{"$ifNull": [{"$arrayElemAt": ["$comments.total", 0]}, 0]}, 1]},  # ëŒ“ê¸€ ê°€ì¤‘ì¹˜ 1
                            {"$multiply": [{"$ifNull": ["$view_count", 0]}, 0.1]}  # ì¡°íšŒìˆ˜ ê°€ì¤‘ì¹˜ 0.1
                        ]
                    }
                }
            },
            {
                "$sort": {"popularity_score": -1}
            },
            {
                "$limit": limit
            },
            {
                "$project": {
                    "reactions": 0,
                    "comments": 0
                }
            }
        ]
        
        return await self.aggregation_service.post_repository.aggregate(pipeline)

# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
class AggregationBenchmark:
    """ì§‘ê³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self, aggregation_service: OptimizedAggregationService):
        self.aggregation_service = aggregation_service
    
    async def benchmark_aggregation_performance(self, sample_size: int = 100) -> Dict[str, Any]:
        """ì§‘ê³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        
        # ìƒ˜í”Œ ê²Œì‹œê¸€ ID ì¡°íšŒ
        sample_posts = await self._get_sample_posts(sample_size)
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        benchmark_results = {}
        
        # 1. ê¸°ì¡´ ë°©ì‹ (ì‹œë®¬ë ˆì´ì…˜)
        benchmark_results["ê¸°ì¡´_ë°©ì‹"] = await self._benchmark_legacy_approach(sample_posts)
        
        # 2. ìµœì í™”ëœ ë°©ì‹
        benchmark_results["ìµœì í™”ëœ_ë°©ì‹"] = await self._benchmark_optimized_approach(sample_posts)
        
        # 3. ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹
        benchmark_results["ë°°ì¹˜_ì²˜ë¦¬"] = await self._benchmark_batch_approach(sample_posts)
        
        return benchmark_results
    
    async def _get_sample_posts(self, sample_size: int) -> List[str]:
        """ìƒ˜í”Œ ê²Œì‹œê¸€ ì¡°íšŒ"""
        
        posts = await self.aggregation_service.post_repository.find_many(
            {"status": "active"},
            limit=sample_size
        )
        
        return [str(post.id) for post in posts]
    
    async def _benchmark_legacy_approach(self, post_ids: List[str]) -> Dict[str, Any]:
        """ê¸°ì¡´ ë°©ì‹ ë²¤ì¹˜ë§ˆí¬"""
        
        times = []
        errors = 0
        
        for post_id in post_ids:
            try:
                start_time = time.time()
                
                # ê¸°ì¡´ ë°©ì‹ ì‹œë®¬ë ˆì´ì…˜ (ì—¬ëŸ¬ ì¿¼ë¦¬ ì‹¤í–‰)
                await self._simulate_legacy_queries(post_id)
                
                end_time = time.time()
                times.append((end_time - start_time) * 1000)
                
            except Exception as e:
                errors += 1
        
        return {
            "ì´_ì‹¤í–‰_ì‹œê°„": sum(times),
            "í‰ê· _ì‹œê°„": sum(times) / len(times) if times else 0,
            "ìµœëŒ€_ì‹œê°„": max(times) if times else 0,
            "ìµœì†Œ_ì‹œê°„": min(times) if times else 0,
            "í‘œì¤€_í¸ì°¨": self._calculate_std_dev(times),
            "ì˜¤ë¥˜_ìˆ˜": errors
        }
    
    async def _benchmark_optimized_approach(self, post_ids: List[str]) -> Dict[str, Any]:
        """ìµœì í™”ëœ ë°©ì‹ ë²¤ì¹˜ë§ˆí¬"""
        
        times = []
        errors = 0
        
        for post_id in post_ids:
            try:
                start_time = time.time()
                
                await self.aggregation_service.get_post_with_everything_optimized(post_id)
                
                end_time = time.time()
                times.append((end_time - start_time) * 1000)
                
            except Exception as e:
                errors += 1
        
        return {
            "ì´_ì‹¤í–‰_ì‹œê°„": sum(times),
            "í‰ê· _ì‹œê°„": sum(times) / len(times) if times else 0,
            "ìµœëŒ€_ì‹œê°„": max(times) if times else 0,
            "ìµœì†Œ_ì‹œê°„": min(times) if times else 0,
            "í‘œì¤€_í¸ì°¨": self._calculate_std_dev(times),
            "ì˜¤ë¥˜_ìˆ˜": errors
        }
    
    async def _benchmark_batch_approach(self, post_ids: List[str]) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ ë²¤ì¹˜ë§ˆí¬"""
        
        start_time = time.time()
        
        try:
            batch_service = BatchAggregationService(self.aggregation_service)
            results = await batch_service.get_multiple_posts_optimized(post_ids)
            
            end_time = time.time()
            total_time = (end_time - start_time) * 1000
            
            return {
                "ì´_ì‹¤í–‰_ì‹œê°„": total_time,
                "í‰ê· _ì‹œê°„": total_time / len(post_ids),
                "ì²˜ë¦¬ëœ_ê²Œì‹œê¸€": len(results),
                "ì²˜ë¦¬_ì‹¤íŒ¨": len(post_ids) - len(results),
                "ë°°ì¹˜_íš¨ìœ¨ì„±": len(results) / len(post_ids) if post_ids else 0
            }
            
        except Exception as e:
            return {
                "ì˜¤ë¥˜": str(e),
                "ì´_ì‹¤í–‰_ì‹œê°„": 0,
                "í‰ê· _ì‹œê°„": 0,
                "ì²˜ë¦¬ëœ_ê²Œì‹œê¸€": 0,
                "ì²˜ë¦¬_ì‹¤íŒ¨": len(post_ids),
                "ë°°ì¹˜_íš¨ìœ¨ì„±": 0
            }
    
    def _calculate_std_dev(self, values: List[float]) -> float:
        """í‘œì¤€í¸ì°¨ ê³„ì‚°"""
        if len(values) < 2:
            return 0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5
    
    async def _simulate_legacy_queries(self, post_id: str):
        """ê¸°ì¡´ ë°©ì‹ ì¿¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜"""
        
        # ì—¬ëŸ¬ ê°œë³„ ì¿¼ë¦¬ ì‹¤í–‰ (ê¸°ì¡´ ë°©ì‹)
        tasks = [
            self.aggregation_service.post_repository.find_one({"_id": ObjectId(post_id)}),
            self.aggregation_service.user_repository.find_one({"_id": ObjectId("507f1f77bcf86cd799439011")}),  # ì˜ˆì‹œ ID
            self.aggregation_service.comment_repository.find_many({"parent_id": post_id}),
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
```

**ì˜ˆìƒ ì¶”ê°€ ìµœì í™” íš¨ê³¼:**
- ì§‘ê³„ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥: ì¶”ê°€ 15-20% í–¥ìƒ
- ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„±: 50-60% í–¥ìƒ
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 30-40% ê°ì†Œ
- ë™ì‹œ ìš”ì²­ ì²˜ë¦¬: 2-3ë°° ì¦ê°€

## ğŸ” 5. ìºì‹œ ë¬´íš¨í™” ì „ëµ

### 5.1 í˜„ì¬ ìºì‹œ ë¬´íš¨í™” ë¬¸ì œì 

**í˜„ì¬ ë¬¸ì œì :**
- ìˆ˜ë™ ìºì‹œ ë¬´íš¨í™”ë¡œ ì¸í•œ ë°ì´í„° ì¼ê´€ì„± ìœ„í—˜
- ì—°ê´€ëœ ìºì‹œ ì‹ë³„ ë° ë¬´íš¨í™” ì–´ë ¤ì›€
- ìºì‹œ ì˜ì¡´ì„± ì¶”ì  ë¶€ì¡±

### 5.2 í–¥ìƒëœ ìºì‹œ ë¬´íš¨í™” ì‹œìŠ¤í…œ

```python
# nadle_backend/services/cache_invalidation_service.py
from typing import Dict, List, Set, Any, Optional
from enum import Enum
import asyncio
from datetime import datetime

class InvalidationTrigger(Enum):
    """ë¬´íš¨í™” íŠ¸ë¦¬ê±° ìœ í˜•"""
    POST_CREATED = "post_created"
    POST_UPDATED = "post_updated"
    POST_DELETED = "post_deleted"
    COMMENT_CREATED = "comment_created"
    COMMENT_UPDATED = "comment_updated"
    COMMENT_DELETED = "comment_deleted"
    USER_UPDATED = "user_updated"
    REACTION_CHANGED = "reaction_changed"
    BOOKMARK_CHANGED = "bookmark_changed"

class CacheInvalidationService:
    """ìºì‹œ ë¬´íš¨í™” ì„œë¹„ìŠ¤"""
    
    def __init__(self, cache_service: SmartCacheService):
        self.cache_service = cache_service
        self.invalidation_rules = self._build_invalidation_rules()
        self.invalidation_queue = asyncio.Queue()
        self.batch_invalidation_size = 50
        self.batch_invalidation_interval = 1.0  # 1ì´ˆ
    
    def _build_invalidation_rules(self) -> Dict[InvalidationTrigger, List[str]]:
        """ë¬´íš¨í™” ê·œì¹™ ì •ì˜"""
        
        return {
            InvalidationTrigger.POST_CREATED: [
                "v1:post:list:*",
                "v1:popular:*",
                "v1:search:*",
                "v1:user:activity:*",
                "v1:stats:*"
            ],
            InvalidationTrigger.POST_UPDATED: [
                "v1:post:detail:{post_id}:*",
                "v1:post:list:*",
                "v1:popular:*",
                "v1:search:*"
            ],
            InvalidationTrigger.POST_DELETED: [
                "v1:post:detail:{post_id}:*",
                "v1:post:list:*",
                "v1:popular:*",
                "v1:search:*",
                "v1:comment:list:{post_id}:*",
                "v1:user:activity:*"
            ],
            InvalidationTrigger.COMMENT_CREATED: [
                "v1:post:detail:{post_id}:*",
                "v1:comment:list:{post_id}:*",
                "v1:user:activity:{user_id}:*",
                "v1:stats:*"
            ],
            InvalidationTrigger.COMMENT_UPDATED: [
                "v1:comment:list:{post_id}:*",
                "v1:post:detail:{post_id}:*"
            ],
            InvalidationTrigger.COMMENT_DELETED: [
                "v1:comment:list:{post_id}:*",
                "v1:post:detail:{post_id}:*",
                "v1:user:activity:{user_id}:*"
            ],
            InvalidationTrigger.USER_UPDATED: [
                "v1:user:profile:{user_id}",
                "v1:post:detail:*",
                "v1:comment:list:*"
            ],
            InvalidationTrigger.REACTION_CHANGED: [
                "v1:post:detail:{post_id}:*",
                "v1:popular:*",
                "v1:user:activity:{user_id}:*"
            ],
            InvalidationTrigger.BOOKMARK_CHANGED: [
                "v1:post:detail:{post_id}:user:{user_id}",
                "v1:user:activity:{user_id}:*"
            ]
        }
    
    async def invalidate_cache(self, trigger: InvalidationTrigger, **context) -> Dict[str, Any]:
        """ìºì‹œ ë¬´íš¨í™” ì‹¤í–‰"""
        
        patterns = self.invalidation_rules.get(trigger, [])
        
        if not patterns:
            return {"invalidated_keys": 0, "patterns": []}
        
        # ì»¨í…ìŠ¤íŠ¸ ë³€ìˆ˜ë¡œ íŒ¨í„´ ì¹˜í™˜
        resolved_patterns = []
        for pattern in patterns:
            try:
                resolved_pattern = pattern.format(**context)
                resolved_patterns.append(resolved_pattern)
            except KeyError:
                # ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë³€ìˆ˜ëŠ” ì™€ì¼ë“œì¹´ë“œë¡œ ìœ ì§€
                resolved_patterns.append(pattern)
        
        # ë°°ì¹˜ ë¬´íš¨í™” íì— ì¶”ê°€
        await self.invalidation_queue.put({
            "trigger": trigger,
            "patterns": resolved_patterns,
            "context": context,
            "timestamp": datetime.now()
        })
        
        return {
            "queued_patterns": len(resolved_patterns),
            "patterns": resolved_patterns
        }
    
    async def process_invalidation_queue(self):
        """ë¬´íš¨í™” í ì²˜ë¦¬"""
        
        batch = []
        
        while True:
            try:
                # ë°°ì¹˜ í¬ê¸°ê¹Œì§€ ìˆ˜ì§‘
                while len(batch) < self.batch_invalidation_size:
                    try:
                        item = await asyncio.wait_for(
                            self.invalidation_queue.get(),
                            timeout=self.batch_invalidation_interval
                        )
                        batch.append(item)
                    except asyncio.TimeoutError:
                        break
                
                if batch:
                    await self._process_batch_invalidation(batch)
                    batch.clear()
                
            except Exception as e:
                logger.error(f"ë¬´íš¨í™” í ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    async def _process_batch_invalidation(self, batch: List[Dict[str, Any]]):
        """ë°°ì¹˜ ë¬´íš¨í™” ì²˜ë¦¬"""
        
        # íŒ¨í„´ ì¤‘ë³µ ì œê±°
        unique_patterns = set()
        for item in batch:
            unique_patterns.update(item["patterns"])
        
        # íŒ¨í„´ë³„ í‚¤ ì¡°íšŒ ë° ë¬´íš¨í™”
        total_invalidated = 0
        
        for pattern in unique_patterns:
            try:
                keys = await self.cache_service.redis.keys(pattern)
                
                if keys:
                    await self.cache_service.redis.delete(*keys)
                    total_invalidated += len(keys)
                
            except Exception as e:
                logger.error(f"íŒ¨í„´ '{pattern}' ë¬´íš¨í™” ì‹¤íŒ¨: {e}")
        
        logger.info(f"ë°°ì¹˜ ë¬´íš¨í™” ì™„ë£Œ: {total_invalidated}ê°œ í‚¤ ë¬´íš¨í™” (íŒ¨í„´: {len(unique_patterns)}ê°œ)")
    
    async def invalidate_user_related_cache(self, user_id: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”"""
        
        patterns = [
            f"v1:user:profile:{user_id}",
            f"v1:user:activity:{user_id}:*",
            f"v1:post:detail:*:user:{user_id}",
            f"v1:comment:list:*"  # ì‚¬ìš©ì ëŒ“ê¸€ì´ í¬í•¨ëœ ëª¨ë“  ëŒ“ê¸€ ëª©ë¡
        ]
        
        total_invalidated = 0
        
        for pattern in patterns:
            keys = await self.cache_service.redis.keys(pattern)
            if keys:
                await self.cache_service.redis.delete(*keys)
                total_invalidated += len(keys)
        
        return {
            "patterns": patterns,
            "invalidated_keys": total_invalidated
        }
    
    async def invalidate_post_related_cache(self, post_id: str) -> Dict[str, Any]:
        """ê²Œì‹œê¸€ ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”"""
        
        patterns = [
            f"v1:post:detail:{post_id}:*",
            f"v1:comment:list:{post_id}:*",
            f"v1:post:list:*",
            f"v1:popular:*",
            f"v1:search:*"
        ]
        
        total_invalidated = 0
        
        for pattern in patterns:
            keys = await self.cache_service.redis.keys(pattern)
            if keys:
                await self.cache_service.redis.delete(*keys)
                total_invalidated += len(keys)
        
        return {
            "patterns": patterns,
            "invalidated_keys": total_invalidated
        }
    
    async def get_invalidation_stats(self) -> Dict[str, Any]:
        """ë¬´íš¨í™” í†µê³„ ì¡°íšŒ"""
        
        # í í¬ê¸°
        queue_size = self.invalidation_queue.qsize()
        
        # ìµœê·¼ ë¬´íš¨í™” ì´ë ¥ (Redisì—ì„œ ì¡°íšŒ)
        recent_invalidations = await self._get_recent_invalidation_history()
        
        return {
            "queue_size": queue_size,
            "recent_invalidations": recent_invalidations,
            "invalidation_rules": len(self.invalidation_rules),
            "batch_size": self.batch_invalidation_size,
            "batch_interval": self.batch_invalidation_interval
        }
    
    async def _get_recent_invalidation_history(self) -> List[Dict[str, Any]]:
        """ìµœê·¼ ë¬´íš¨í™” ì´ë ¥ ì¡°íšŒ"""
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Redisë‚˜ ë¡œê·¸ì—ì„œ ì¡°íšŒ
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ë°˜í™˜
        return []

class EventDrivenCacheInvalidation:
    """ì´ë²¤íŠ¸ ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”"""
    
    def __init__(self, invalidation_service: CacheInvalidationService):
        self.invalidation_service = invalidation_service
        self.event_subscribers: Dict[str, List[callable]] = {}
    
    def subscribe(self, event_type: str, callback: callable):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        if event_type not in self.event_subscribers:
            self.event_subscribers[event_type] = []
        
        self.event_subscribers[event_type].append(callback)
    
    async def publish_event(self, event_type: str, **data):
        """ì´ë²¤íŠ¸ ë°œí–‰"""
        
        subscribers = self.event_subscribers.get(event_type, [])
        
        if subscribers:
            tasks = []
            for callback in subscribers:
                task = asyncio.create_task(callback(**data))
                tasks.append(task)
            
            await asyncio.gather(*tasks, return_exceptions=True)
    
    async def setup_default_subscribers(self):
        """ê¸°ë³¸ êµ¬ë…ì ì„¤ì •"""
        
        # ê²Œì‹œê¸€ ì´ë²¤íŠ¸ êµ¬ë…
        self.subscribe("post_created", self._handle_post_created)
        self.subscribe("post_updated", self._handle_post_updated)
        self.subscribe("post_deleted", self._handle_post_deleted)
        
        # ëŒ“ê¸€ ì´ë²¤íŠ¸ êµ¬ë…
        self.subscribe("comment_created", self._handle_comment_created)
        self.subscribe("comment_updated", self._handle_comment_updated)
        self.subscribe("comment_deleted", self._handle_comment_deleted)
        
        # ì‚¬ìš©ì ì´ë²¤íŠ¸ êµ¬ë…
        self.subscribe("user_updated", self._handle_user_updated)
        
        # ë°˜ì‘ ì´ë²¤íŠ¸ êµ¬ë…
        self.subscribe("reaction_changed", self._handle_reaction_changed)
        self.subscribe("bookmark_changed", self._handle_bookmark_changed)
    
    async def _handle_post_created(self, **data):
        """ê²Œì‹œê¸€ ìƒì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        await self.invalidation_service.invalidate_cache(
            InvalidationTrigger.POST_CREATED,
            **data
        )
    
    async def _handle_post_updated(self, **data):
        """ê²Œì‹œê¸€ ìˆ˜ì • ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        await self.invalidation_service.invalidate_cache(
            InvalidationTrigger.POST_UPDATED,
            **data
        )
    
    async def _handle_post_deleted(self, **data):
        """ê²Œì‹œê¸€ ì‚­ì œ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        await self.invalidation_service.invalidate_cache(
            InvalidationTrigger.POST_DELETED,
            **data
        )
    
    async def _handle_comment_created(self, **data):
        """ëŒ“ê¸€ ìƒì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        await self.invalidation_service.invalidate_cache(
            InvalidationTrigger.COMMENT_CREATED,
            **data
        )
    
    async def _handle_comment_updated(self, **data):
        """ëŒ“ê¸€ ìˆ˜ì • ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        await self.invalidation_service.invalidate_cache(
            InvalidationTrigger.COMMENT_UPDATED,
            **data
        )
    
    async def _handle_comment_deleted(self, **data):
        """ëŒ“ê¸€ ì‚­ì œ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        await self.invalidation_service.invalidate_cache(
            InvalidationTrigger.COMMENT_DELETED,
            **data
        )
    
    async def _handle_user_updated(self, **data):
        """ì‚¬ìš©ì ì •ë³´ ìˆ˜ì • ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        await self.invalidation_service.invalidate_cache(
            InvalidationTrigger.USER_UPDATED,
            **data
        )
    
    async def _handle_reaction_changed(self, **data):
        """ë°˜ì‘ ë³€ê²½ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        await self.invalidation_service.invalidate_cache(
            InvalidationTrigger.REACTION_CHANGED,
            **data
        )
    
    async def _handle_bookmark_changed(self, **data):
        """ë¶ë§ˆí¬ ë³€ê²½ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        await self.invalidation_service.invalidate_cache(
            InvalidationTrigger.BOOKMARK_CHANGED,
            **data
        )

# ì‚¬ìš© ì˜ˆì‹œ
async def setup_cache_invalidation():
    """ìºì‹œ ë¬´íš¨í™” ì‹œìŠ¤í…œ ì„¤ì •"""
    
    # ìºì‹œ ì„œë¹„ìŠ¤ ìƒì„±
    cache_service = SmartCacheService(redis_manager)
    
    # ë¬´íš¨í™” ì„œë¹„ìŠ¤ ìƒì„±
    invalidation_service = CacheInvalidationService(cache_service)
    
    # ì´ë²¤íŠ¸ ê¸°ë°˜ ë¬´íš¨í™” ìƒì„±
    event_invalidation = EventDrivenCacheInvalidation(invalidation_service)
    await event_invalidation.setup_default_subscribers()
    
    # ë¬´íš¨í™” í ì²˜ë¦¬ ì‹œì‘
    asyncio.create_task(invalidation_service.process_invalidation_queue())
    
    return invalidation_service, event_invalidation

# ì„œë¹„ìŠ¤ ë ˆì´ì–´ì—ì„œ ì‚¬ìš©
class PostService:
    def __init__(self, event_invalidation: EventDrivenCacheInvalidation):
        self.event_invalidation = event_invalidation
    
    async def create_post(self, post_data: Dict[str, Any]) -> Dict[str, Any]:
        """ê²Œì‹œê¸€ ìƒì„±"""
        
        # ê²Œì‹œê¸€ ìƒì„± ë¡œì§
        post = await self.post_repository.create(post_data)
        
        # ì´ë²¤íŠ¸ ë°œí–‰ (ìë™ ìºì‹œ ë¬´íš¨í™”)
        await self.event_invalidation.publish_event(
            "post_created",
            post_id=str(post.id),
            user_id=str(post.author_id),
            category=post.metadata.get("category")
        )
        
        return post
    
    async def update_post(self, post_id: str, update_data: Dict[str, Any]) -> Dict[str, Any]:
        """ê²Œì‹œê¸€ ìˆ˜ì •"""
        
        # ê²Œì‹œê¸€ ìˆ˜ì • ë¡œì§
        post = await self.post_repository.update(post_id, update_data)
        
        # ì´ë²¤íŠ¸ ë°œí–‰ (ìë™ ìºì‹œ ë¬´íš¨í™”)
        await self.event_invalidation.publish_event(
            "post_updated",
            post_id=post_id,
            user_id=str(post.author_id)
        )
        
        return post
```

**ì˜ˆìƒ ìºì‹œ ë¬´íš¨í™” ìµœì í™” íš¨ê³¼:**
- ë°ì´í„° ì¼ê´€ì„±: 99.9% ë³´ì¥
- ë¬´íš¨í™” ì§€ì—° ì‹œê°„: 90% ê°ì†Œ
- ì‹œìŠ¤í…œ ë³µì¡ë„: ëŒ€í­ ë‹¨ìˆœí™”
- ê°œë°œ ìƒì‚°ì„±: 50% í–¥ìƒ

## ğŸ¯ ë°ì´í„°ë² ì´ìŠ¤ ë° ìºì‹± ìµœì í™” ì‹¤í–‰ ê³„íš

### Phase 1: Critical Infrastructure (1-2ì£¼)
1. **MongoDB ì—°ê²° í’€ ìµœì í™”** - ì•ˆì •ì„± í™•ë³´
2. **í•µì‹¬ ì¸ë±ìŠ¤ ìƒì„±** - ì¦‰ì‹œ ì„±ëŠ¥ í–¥ìƒ
3. **Redis ë©”ëª¨ë¦¬ ìµœì í™”** - ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±

### Phase 2: Performance Enhancement (2-3ì£¼)
1. **ìŠ¤ë§ˆíŠ¸ ìºì‹± ì „ëµ êµ¬í˜„** - ìºì‹œ íš¨ìœ¨ì„± í–¥ìƒ
2. **ì§‘ê³„ íŒŒì´í”„ë¼ì¸ ì¶”ê°€ ìµœì í™”** - ì¿¼ë¦¬ ì„±ëŠ¥ ê°œì„ 
3. **ì—°ê²° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•** - ì„±ëŠ¥ ì¶”ì 

### Phase 3: Advanced Features (1-2ê°œì›”)
1. **ì´ë²¤íŠ¸ ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”** - ë°ì´í„° ì¼ê´€ì„± í™•ë³´
2. **ìºì‹œ ì›Œë° ì‹œìŠ¤í…œ** - ì‚¬ìš©ì ê²½í—˜ ê°œì„ 
3. **ì„±ëŠ¥ ìë™ ìµœì í™”** - ìë™í™”ëœ ì„±ëŠ¥ ê´€ë¦¬

## ğŸ“Š ì˜ˆìƒ ì¢…í•© íš¨ê³¼

### ë‹¨ê¸° íš¨ê³¼ (Phase 1-2 ì™„ë£Œ)
- **ì¿¼ë¦¬ ì„±ëŠ¥**: 50-70% í–¥ìƒ
- **ìºì‹œ íš¨ìœ¨**: 40-60% í–¥ìƒ
- **ì‹œìŠ¤í…œ ì•ˆì •ì„±**: ëŒ€í­ í–¥ìƒ
- **ë™ì‹œ ì‚¬ìš©ì ì²˜ë¦¬**: 3-5ë°° ì¦ê°€

### ì¤‘ì¥ê¸° íš¨ê³¼ (ì „ì²´ ì™„ë£Œ)
- **ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥**: 70-90% í–¥ìƒ
- **ìºì‹œ ì‹œìŠ¤í…œ íš¨ìœ¨**: 60-80% í–¥ìƒ
- **ì „ì²´ ì‘ë‹µ ì‹œê°„**: 50-70% ë‹¨ì¶•
- **ë¦¬ì†ŒìŠ¤ ì‚¬ìš© íš¨ìœ¨**: 40-60% í–¥ìƒ
- **ì‹œìŠ¤í…œ í™•ì¥ì„±**: 10ë°° í–¥ìƒ

ì´ëŸ¬í•œ ë°ì´í„°ë² ì´ìŠ¤ ë° ìºì‹± ìµœì í™”ë¥¼ í†µí•´ XAI Community v5ì˜ ë°±ì—”ë“œ ì„±ëŠ¥ì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ê³ , ëŒ€ê·œëª¨ íŠ¸ë˜í”½ì—ë„ ì•ˆì •ì ìœ¼ë¡œ ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.